{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L42aSQXW0XP"
   },
   "source": [
    "# Retrival test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A35XjxiBYxA5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install qdrant_client\n",
    "!pip install sentence_transformers\n",
    "!pip install langchain-community\n",
    "!pip install replicate\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install langchain pydantic\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = \"\"\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEvACI21ZHZu"
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "\n",
    "cluster='llm4eo'\n",
    "COLLECTION_NAME = \"esa-data-qwen-1024\"\n",
    "\n",
    "\n",
    "if cluster=='llm4eo':\n",
    "    QDRANT_API_KEY=os.getenv('QDRANT_API_KEY_1')\n",
    "    QDRANT_URL=os.getenv('QDRANT_URL_1')        \n",
    "else:\n",
    "    QDRANT_API_KEY=os.getenv('QDRANT_API_KEY')\n",
    "    QDRANT_URL=os.getenv('QDRANT_URL')    \n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    timeout=120\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ypmnx18qWtOT"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class qwen_embedder:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen3-Embedding-4B\"):\n",
    "        # Load the sentence-transformers model\n",
    "        self.model = SentenceTransformer(\n",
    "                                    model_name,\n",
    "                                    model_kwargs={\n",
    "                                        \"torch_dtype\": \"auto\",       # important: will use float16/bfloat16 automatically\n",
    "                                        \"device_map\": \"auto\",\n",
    "                                    },\n",
    "                                    tokenizer_kwargs={\"padding_side\": \"left\",\n",
    "                                                      \"max_length\": 2048,\n",
    "                                                      \"truncation\": True\n",
    "                                                      }\n",
    "                                                      )\n",
    "\n",
    "    def embed_documents(self,\n",
    "                        texts,\n",
    "                        batch_size=8,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=2048,\n",
    "                        normalize=True):\n",
    "        \"\"\"\n",
    "        Encodes a list of texts into embeddings.\n",
    "\n",
    "        Args:\n",
    "            texts (list[str]): Documents to embed\n",
    "            padding (bool/str): True = dynamic padding, 'max_length' = fixed length\n",
    "            truncation (bool): Whether to truncate texts beyond max_length\n",
    "            max_length (int): Max tokens allowed\n",
    "            normalize (bool): Whether to L2 normalize embeddings\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Embeddings array (num_texts x embedding_dim)\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            normalize_embeddings=normalize,\n",
    "            convert_to_numpy=True,\n",
    "            convert_to_tensor=False\n",
    "        )\n",
    "        embeddings = embeddings.tolist()\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def embed_query(self,query):\n",
    "\n",
    "        embeddings = self.model.encode( query,prompt_name=\"query\")\n",
    "\n",
    "        embeddings = embeddings.tolist()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "class IndusEmbedder:\n",
    "    def __init__(self, model_name: str = \"Tulsikumar/indus-sde-st-v0.2\", device: str = None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def embed_documents(self, documents: list[str], batch_size: int = 100) -> list[list[float]]:\n",
    "        \"\"\"Embed a batch of documents (list of strings) in smaller chunks and return a list of vectors.\"\"\"\n",
    "        all_embeddings = []\n",
    "\n",
    "    \n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i : i + batch_size]\n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "            embeddings = self._mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "            all_embeddings.extend(embeddings.cpu().tolist())  # convert to list of lists\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def embed_query(self, query: str) -> list[float]:\n",
    "        \"\"\"Embed a single query string and return a list of floats.\"\"\"\n",
    "        return self.embed_documents([query], batch_size=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2  # seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U4vLPdTnWtLJ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd347e89dd240bcb9249be4510274f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06c15f28a314fb3a3dcfffadc3a1394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ca24159c7f4032949742f7e21201b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7520f4f38ec64518b583fa52c68f9a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6154686ef994370a9290de2ef078495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bf909e70ef428499a955400735a0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c6a92410c2412e9b06c9cedfdbad74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5fe1e7d10c449e9b901695424e662a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943e85ca6d4c46a7b84e1534fefaf4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if COLLECTION_NAME == 'esa-data-indus' or COLLECTION_NAME == 'esa-data-indus-quant' or COLLECTION_NAME =='esa-data-indus-512-1024':\n",
    "    model_name=\"nasa-impact/nasa-smd-ibm-st-v2\"\n",
    "    normalize=True\n",
    "    encode_kwargs = {\"normalize_embeddings\": normalize}\n",
    "    embedder=HuggingFaceEmbeddings(model_name=model_name, encode_kwargs=encode_kwargs)\n",
    "elif COLLECTION_NAME == 'esa-data-qwen' or COLLECTION_NAME == 'esa-data-qwen-quant' or COLLECTION_NAME =='esa-data-qwen-1024':\n",
    "    embedder=qwen_embedder(model_name=\"Qwen/Qwen3-Embedding-4B\")\n",
    "\n",
    "elif COLLECTION_NAME == \"esa-data-indus-1024\" or COLLECTION_NAME == \"esa-data-indus-1024-quant\":\n",
    "    model_name=\"Tulsikumar/indus-sde-st-v0.2\"\n",
    "    normalize=True\n",
    "    encode_kwargs = {\"normalize_embeddings\": normalize}\n",
    "    #embedder=HuggingFaceEmbeddings(model_name=model_name, encode_kwargs=encode_kwargs)\n",
    "    embedder=IndusEmbedder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "K = 20\n",
    "csv_path = \"data_with_unique_questions.csv\"\n",
    "df_queries = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zTYLanJgWtEm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing retrieval:   0%|          | 0/1140 [00:00<?, ?it/s]/tmp/ipykernel_1161/4012111910.py:15: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client.search(\n",
      "Testing retrieval: 100%|██████████| 1140/1140 [36:59<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank distribution:\n",
      "Rank 1: 0 (0.00%)\n",
      "Rank 2: 0 (0.00%)\n",
      "Rank 3: 0 (0.00%)\n",
      "Rank 4: 0 (0.00%)\n",
      "Rank 5: 0 (0.00%)\n",
      "Rank 6: 0 (0.00%)\n",
      "Rank 7: 0 (0.00%)\n",
      "Rank 8: 0 (0.00%)\n",
      "Rank 9: 0 (0.00%)\n",
      "Rank 10: 0 (0.00%)\n",
      "Rank 11: 0 (0.00%)\n",
      "Rank 12: 0 (0.00%)\n",
      "Rank 13: 0 (0.00%)\n",
      "Rank 14: 0 (0.00%)\n",
      "Rank 15: 0 (0.00%)\n",
      "Rank 16: 0 (0.00%)\n",
      "Rank 17: 0 (0.00%)\n",
      "Rank 18: 0 (0.00%)\n",
      "Rank 19: 0 (0.00%)\n",
      "Rank 20: 0 (0.00%)\n",
      "Rank not_in_topk: 1140 (100.00%)\n",
      "\n",
      "Saved results including retrieval time and retrieved text to llm_qa_qwen_1024_test_20.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "rank_counts = {i: 0 for i in range(1, K+1)}\n",
    "rank_counts[\"not_in_topk\"] = 0\n",
    "\n",
    "for _, row in tqdm(df_queries.iterrows(), total=len(df_queries), desc=\"Testing retrieval\"):\n",
    "    chunk_id = row[\"id\"]\n",
    "    query_text = row[\"question\"]\n",
    "    references = row[\"references\"] if pd.notna(row[\"references\"]) else []\n",
    "#    references = row['']\n",
    "\n",
    "    query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "    # Measure retrieval time\n",
    "    start_time = time.time()\n",
    "    search_result = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_vector,\n",
    "        limit=K,\n",
    "        with_payload=True,\n",
    "        search_params=models.SearchParams(\n",
    "                quantization=models.QuantizationSearchParams(\n",
    "                    ignore=False,\n",
    "                    rescore=True,\n",
    "                    oversampling=4.0,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    retrieval_time = time.time() - start_time  # in seconds\n",
    "\n",
    "    retrieved_ids = [res.id for res in search_result]\n",
    "    retrieved_scores = [res.score for res in search_result]\n",
    "\n",
    "    if chunk_id in retrieved_ids:\n",
    "        rank = retrieved_ids.index(chunk_id) + 1\n",
    "        score_at_rank = retrieved_scores[rank - 1]\n",
    "        rank_counts[rank] += 1\n",
    "    else:\n",
    "        rank = None\n",
    "        score_at_rank = None\n",
    "        rank_counts[\"not_in_topk\"] += 1\n",
    "\n",
    "    # Get top chunk content and compute coverage\n",
    "    if search_result:\n",
    "        retrieved_texts = [res.payload.get(\"content\", \"\") for res in search_result]\n",
    "        sep = \"<DOC_SEP>\"\n",
    "        retrieved_text = sep.join(retrieved_texts)\n",
    "    \n",
    "        # Check each reference in all chunks\n",
    "        found_refs = 0\n",
    "        for ref in references:\n",
    "            if any(ref in chunk for chunk in retrieved_texts):\n",
    "                found_refs += 1\n",
    "    \n",
    "        coverage = found_refs / len(references) if references else 0\n",
    "    else:\n",
    "        retrieved_text = \"\"\n",
    "        coverage = 0\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"doc_id\": chunk_id,\n",
    "        \"query_text\": query_text,\n",
    "        \"references\":references,\n",
    "        \"retrieved_ids\": retrieved_ids,\n",
    "        \"retrieved_scores\": retrieved_scores,\n",
    "        \"retrieved_text\": retrieved_text,\n",
    "        \"coverage\": coverage,\n",
    "        \"retrieval_time\": retrieval_time,\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nRank distribution:\")\n",
    "for r, c in rank_counts.items():\n",
    "    pct = (c / len(df_queries))*100\n",
    "    print(f\"Rank {r}: {c} ({pct:.2f}%)\")\n",
    "\n",
    "# Save results\n",
    "output_file = \"llm_qa_qwen_1024_test_20.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"\\nSaved results including retrieval time and retrieved text to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "from typing import List, Dict\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def is_reference_present_fuzzy(reference: str, document: str, threshold: float = 0.8) -> bool:\n",
    "    ref_tokens = normalize_text(reference).split()\n",
    "    doc_tokens = normalize_text(document).split()\n",
    "    if not ref_tokens:\n",
    "        return False\n",
    "    matched_tokens = sum(1 for t in ref_tokens if t in doc_tokens)\n",
    "    fraction_matched = matched_tokens / len(ref_tokens)\n",
    "    return fraction_matched >= threshold\n",
    "\n",
    "def compute_token_metrics_single_doc(\n",
    "    references: List[str],\n",
    "    retrieved_texts: List[str],\n",
    "    threshold: float = 0.8\n",
    ") -> Dict[str, float]:\n",
    "    all_ref_tokens = []\n",
    "    matched_tokens = []\n",
    "    found_count = 0\n",
    "\n",
    "    for ref in references:\n",
    "        ref_tokens = normalize_text(ref).split()\n",
    "        all_ref_tokens.extend(ref_tokens)\n",
    "\n",
    "        matched_docs = [doc for doc in retrieved_texts if is_reference_present_fuzzy(ref, doc, threshold)]\n",
    "        if matched_docs:\n",
    "            found_count += 1\n",
    "            for doc in matched_docs:\n",
    "                matched_tokens.extend(normalize_text(doc).split())\n",
    "\n",
    "    all_doc_tokens = []\n",
    "    for doc in retrieved_texts:\n",
    "        all_doc_tokens.extend(normalize_text(doc).split())\n",
    "\n",
    "    if not all_ref_tokens:\n",
    "        return {\"iou\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"ref_found_ratio\": 0.0}\n",
    "\n",
    "    ref_counter = Counter(all_ref_tokens)\n",
    "    match_counter = Counter(matched_tokens)\n",
    "    doc_counter = Counter(all_doc_tokens)\n",
    "\n",
    "    intersection_count = sum((ref_counter & match_counter).values())\n",
    "    ref_count = sum(ref_counter.values())\n",
    "    doc_count = sum(doc_counter.values())\n",
    "    union_count = ref_count + doc_count - intersection_count\n",
    "\n",
    "    iou = intersection_count / union_count if union_count > 0 else 0.0\n",
    "    precision = intersection_count / doc_count if doc_count > 0 else 0.0\n",
    "    recall = intersection_count / ref_count if ref_count > 0 else 0.0\n",
    "    ref_found_ratio = found_count / len(references) if references else 0.0\n",
    "\n",
    "    return {\n",
    "        \"iou\": iou,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"ref_found_ratio\": ref_found_ratio,\n",
    "    }\n",
    "\n",
    "def compute_metrics_single_row(\n",
    "    ref_str: str,\n",
    "    ret_str: str,\n",
    "    K: int,\n",
    "    ref_sep: str = \"|\",\n",
    "    doc_sep: str = \"<DOC_SEP>\",\n",
    "    token_threshold: float = 0.95,\n",
    "    rr_threshold: float = 1.0\n",
    ") -> Dict[str, float]:\n",
    "    references = [r.strip() for r in str(ref_str).split(ref_sep) if r.strip()]\n",
    "    retrieved_texts = [doc.strip() for doc in str(ret_str).split(doc_sep) if doc.strip()]\n",
    "\n",
    "    metrics = compute_token_metrics_single_doc(references, retrieved_texts[:K], threshold=token_threshold)\n",
    "\n",
    "    rr = 0.0\n",
    "    found_rank = None\n",
    "    for rank, doc in enumerate(retrieved_texts[:K], start=1):\n",
    "        if any(is_reference_present_fuzzy(ref, doc, threshold=rr_threshold) for ref in references):\n",
    "            rr = 1.0 / rank\n",
    "            found_rank = rank\n",
    "            break\n",
    "\n",
    "    metrics[\"reciprocal_rank\"] = rr\n",
    "    metrics[\"ref_rank\"] = found_rank  # None if not found\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "top_k = 5\n",
    "metrics_list = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing metrics\"):\n",
    "    metrics = compute_metrics_single_row(row[\"references\"], row[\"retrieved_text\"], top_k)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_list)\n",
    "df = pd.concat([df, df_metrics], axis=1)\n",
    "\n",
    "average_metrics = df_metrics.mean().to_dict()\n",
    "print(f\"\\nAverage metrics across all rows @{top_k}:\")\n",
    "print(average_metrics)\n",
    "\n",
    "ref_tok_10 = average_metrics['ref_found_ratio']\n",
    "mrr_tok_10 = average_metrics['reciprocal_rank']\n",
    "\n",
    "rank_counts = df_metrics[\"ref_rank\"].value_counts().sort_index()\n",
    "rank_percents = (rank_counts / len(df_metrics)) * 100\n",
    "\n",
    "print(\"\\nRank distribution:\")\n",
    "for r, c in rank_counts.items():\n",
    "    pct = rank_percents[r]\n",
    "    print(f\"Rank {r}: {c} docs ({pct:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gRDDaXMycRpq",
    "yohA-Pq-cari"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
