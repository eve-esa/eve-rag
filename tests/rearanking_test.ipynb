{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbY-VE3AXfst",
    "outputId": "ac5b50a5-eeef-46dd-c869-c1acca3df5fa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoModelForSequenceClassification\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "\n",
    "class Qwen3Reranker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen3-Reranker-4B\",\n",
    "        max_length: int = 2048,\n",
    "        device: Optional[str] = None,\n",
    "        dtype: torch.dtype = torch.float16\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=dtype\n",
    "        ).eval()\n",
    "\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.token_false_id = self.tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self.token_true_id = self.tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.prefix = (\n",
    "            \"<|im_start|>system\\n\"\n",
    "            \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "            'Note that the answer can only be \"yes\" or \"no\".<|im_end|>\\n<|im_start|>user\\n'\n",
    "        )\n",
    "        self.suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "    def _format_instruction(self, query: str, doc: str, instruction: Optional[str] = None) -> str:\n",
    "        if instruction is None:\n",
    "            instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "        return f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "    def _tokenize_batch(self, texts: List[str]):\n",
    "        \"\"\"Batch tokenization for speed.\"\"\"\n",
    "        modified_texts = [self.prefix + t + self.suffix for t in texts]\n",
    "        inputs = self.tokenizer(\n",
    "            modified_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _score_single(self, input_ids, attention_mask) -> float:\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits[:, -1, :]\n",
    "        true_vector = logits[:, self.token_true_id]\n",
    "        false_vector = logits[:, self.token_false_id]\n",
    "        scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "        scores = torch.nn.functional.log_softmax(scores, dim=1)\n",
    "        prob_yes = scores[:, 1].exp().item()\n",
    "        return prob_yes\n",
    "\n",
    "    def score_pairs(self, queries: List[str], docs: List[str], instruction: Optional[str] = None) -> List[float]:\n",
    "        pairs = [self._format_instruction(q, d, instruction) for q, d in zip(queries, docs)]\n",
    "        inputs = self._tokenize_batch(pairs)\n",
    "\n",
    "        scores = []\n",
    "        for i in range(len(docs)):\n",
    "            score = self._score_single(\n",
    "                inputs['input_ids'][i].unsqueeze(0),\n",
    "                inputs['attention_mask'][i].unsqueeze(0)\n",
    "            )\n",
    "            scores.append(score)\n",
    "            torch.cuda.empty_cache()  # free memory after each forward\n",
    "        return scores\n",
    "\n",
    "    def rerank(self, query: str, docs: List[str], instruction: Optional[str] = None) -> tuple[list[str], list[float]]:\n",
    "        queries = [query] * len(docs)\n",
    "        scores = self.score_pairs(queries, docs, instruction)\n",
    "        ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        if ranked:\n",
    "            ranked_docs, ranked_scores = zip(*ranked)\n",
    "        else:\n",
    "            ranked_docs, ranked_scores = [], []\n",
    "        return list(ranked_docs), list(ranked_scores)\n",
    "\n",
    "    def rerank_dict(self, query: str, docs: List[str], instruction: Optional[str] = None) -> List[dict]:\n",
    "        ranked_docs, ranked_scores = self.rerank(query, docs, instruction)\n",
    "        return [{\"doc\": d, \"score\": s} for d, s in zip(ranked_docs, ranked_scores)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class indus_reranker:\n",
    "    def __init__(self, model_name=\"nasa-impact/nasa-smd-ibm-ranker\", device=None, max_length=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def rerank(self, query, docs, debug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (str): The query string\n",
    "            docs (list of str): List of documents to rerank\n",
    "            debug (bool): If True, print debug info\n",
    "        Returns:\n",
    "            tuple: (sorted_docs, scores)\n",
    "                - sorted_docs: list of documents sorted by relevance\n",
    "                - scores: list of corresponding probabilities\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Tokenize query-doc pairs\n",
    "        encodings = self.tokenizer(\n",
    "            text=[query] * len(docs),      # query repeated\n",
    "            text_pair=docs,                # each doc as the pair\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "        # Compute logits and probabilities\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**encodings).logits\n",
    "\n",
    "            if debug:\n",
    "                print(\"Logits shape:\", logits.shape)\n",
    "                print(\"Sample logits:\", logits[:5].cpu().numpy())\n",
    "\n",
    "            # Handle binary classification (2 logits) or single logit (sigmoid)\n",
    "            if logits.shape[1] == 2:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)[:, 1]  # positive class\n",
    "            else:  # shape [N, 1] → use sigmoid\n",
    "                probs = torch.sigmoid(logits).squeeze(-1)\n",
    "\n",
    "            if debug:\n",
    "                print(\"Sample probs:\", probs[:5].cpu().numpy())\n",
    "\n",
    "        # Sort by probability descending\n",
    "        sorted_pairs = sorted(zip(docs, probs.cpu().tolist()), key=lambda x: x[1], reverse=True)\n",
    "        sorted_docs, scores = zip(*sorted_pairs)\n",
    "\n",
    "        reranking_time = time.time() - start_time\n",
    "        if debug:\n",
    "            print(f\"Reranking took {reranking_time:.4f} seconds\")\n",
    "            print(\"Sorted docs (top 3):\", sorted_docs[:3])\n",
    "            print(\"Sorted scores (top 3):\", scores[:3])\n",
    "\n",
    "        return list(sorted_docs), list(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reranker =indus_reranker(max_length=512)\n",
    "reranker=Qwen3Reranker(max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking rows: 100%|██████████| 1140/1140 [02:10<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked results saved to reranked_qwen_1024_indus_20.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_csv = \"\" # file with retrieved text\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "df[\"reranked_docs\"] = None\n",
    "df[\"reranked_scores\"] = None\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Reranking rows\"):\n",
    "    query = row[\"query_text\"]\n",
    "    sep = \"<DOC_SEP>\"\n",
    "    # Convert pipe-separated string into list of docs\n",
    "    retrieved_texts = [doc.strip() for doc in row[\"retrieved_text\"].split(sep)]\n",
    "\n",
    "    #print(\"Number of retrieved docs:\", len(retrieved_texts))\n",
    "\n",
    "    # Rerank\n",
    "    ranked_docs, ranked_scores = reranker.rerank(query, retrieved_texts)\n",
    "\n",
    "    # Save back to dataframe\n",
    "    df.at[idx, \"reranked_docs\"] = sep.join(ranked_docs)\n",
    "    df.at[idx, \"reranked_scores\"] = ranked_scores\n",
    "\n",
    "\n",
    "output_csv = \"reranked_qwen_1024_indus_20.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Reranked results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "from typing import List, Dict\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def is_reference_present_fuzzy(reference: str, document: str, threshold: float = 0.8) -> bool:\n",
    "    ref_tokens = normalize_text(reference).split()\n",
    "    doc_tokens = normalize_text(document).split()\n",
    "    if not ref_tokens:\n",
    "        return False\n",
    "    matched_tokens = sum(1 for t in ref_tokens if t in doc_tokens)\n",
    "    fraction_matched = matched_tokens / len(ref_tokens)\n",
    "    return fraction_matched >= threshold\n",
    "\n",
    "def compute_token_metrics_single_doc(\n",
    "    references: List[str],\n",
    "    retrieved_texts: List[str],\n",
    "    threshold: float = 0.8\n",
    ") -> Dict[str, float]:\n",
    "    all_ref_tokens = []\n",
    "    matched_tokens = []\n",
    "    found_count = 0\n",
    "\n",
    "    for ref in references:\n",
    "        ref_tokens = normalize_text(ref).split()\n",
    "        all_ref_tokens.extend(ref_tokens)\n",
    "\n",
    "        matched_docs = [doc for doc in retrieved_texts if is_reference_present_fuzzy(ref, doc, threshold)]\n",
    "        if matched_docs:\n",
    "            found_count += 1\n",
    "            for doc in matched_docs:\n",
    "                matched_tokens.extend(normalize_text(doc).split())\n",
    "\n",
    "    all_doc_tokens = []\n",
    "    for doc in retrieved_texts:\n",
    "        all_doc_tokens.extend(normalize_text(doc).split())\n",
    "\n",
    "    if not all_ref_tokens:\n",
    "        return {\"iou\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"ref_found_ratio\": 0.0}\n",
    "\n",
    "    ref_counter = Counter(all_ref_tokens)\n",
    "    match_counter = Counter(matched_tokens)\n",
    "    doc_counter = Counter(all_doc_tokens)\n",
    "\n",
    "    intersection_count = sum((ref_counter & match_counter).values())\n",
    "    ref_count = sum(ref_counter.values())\n",
    "    doc_count = sum(doc_counter.values())\n",
    "    union_count = ref_count + doc_count - intersection_count\n",
    "\n",
    "    iou = intersection_count / union_count if union_count > 0 else 0.0\n",
    "    precision = intersection_count / doc_count if doc_count > 0 else 0.0\n",
    "    recall = intersection_count / ref_count if ref_count > 0 else 0.0\n",
    "    ref_found_ratio = found_count / len(references) if references else 0.0\n",
    "\n",
    "    return {\n",
    "        \"iou\": iou,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"ref_found_ratio\": ref_found_ratio,\n",
    "    }\n",
    "\n",
    "def compute_metrics_single_row(\n",
    "    ref_str: str,\n",
    "    ret_str: str,\n",
    "    K: int,\n",
    "    ref_sep: str = \"|\",\n",
    "    doc_sep: str = \"<DOC_SEP>\",\n",
    "    token_threshold: float = 0.95,\n",
    "    rr_threshold: float = 1.0\n",
    ") -> Dict[str, float]:\n",
    "    references = [r.strip() for r in str(ref_str).split(ref_sep) if r.strip()]\n",
    "    retrieved_texts = [doc.strip() for doc in str(ret_str).split(doc_sep) if doc.strip()]\n",
    "\n",
    "    metrics = compute_token_metrics_single_doc(references, retrieved_texts[:K], threshold=token_threshold)\n",
    "\n",
    "    rr = 0.0\n",
    "    found_rank = None\n",
    "    for rank, doc in enumerate(retrieved_texts[:K], start=1):\n",
    "        if any(is_reference_present_fuzzy(ref, doc, threshold=rr_threshold) for ref in references):\n",
    "            rr = 1.0 / rank\n",
    "            found_rank = rank\n",
    "            break\n",
    "\n",
    "    metrics[\"reciprocal_rank\"] = rr\n",
    "    metrics[\"ref_rank\"] = found_rank  # None if not found\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_csv)\n",
    "\n",
    "top_k = 10\n",
    "metrics_list = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing metrics\"):\n",
    "    metrics = compute_metrics_single_row(row[\"references\"], row[\"reranked_docs\"], top_k)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_list)\n",
    "df = pd.concat([df, df_metrics], axis=1)\n",
    "\n",
    "average_metrics = df_metrics.mean().to_dict()\n",
    "print(f\"\\nAverage metrics across all rows @{top_k}:\")\n",
    "print(average_metrics)\n",
    "\n",
    "ref_tok_10 = average_metrics['ref_found_ratio']\n",
    "mrr_tok_10 = average_metrics['reciprocal_rank']\n",
    "\n",
    "rank_counts = df_metrics[\"ref_rank\"].value_counts().sort_index()\n",
    "rank_percents = (rank_counts / len(df_metrics)) * 100\n",
    "\n",
    "print(\"\\nRank distribution:\")\n",
    "for r, c in rank_counts.items():\n",
    "    pct = rank_percents[r]\n",
    "    print(f\"Rank {r}: {c} docs ({pct:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "153d09b695ba4724a5c53f8845e0704d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1696edf8397e4ce8bcfd059b8a793c85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f32abfca153a4c1aacfd15903a40b935",
      "placeholder": "​",
      "style": "IPY_MODEL_153d09b695ba4724a5c53f8845e0704d",
      "value": " 1/5 [00:16&lt;01:04, 16.20s/it]"
     }
    },
    "19d16c3d1dad44aab83908458c3e4387": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3111779021c2433c97372496634a7506",
       "IPY_MODEL_3938eb4a901143f297155d1107e388a7",
       "IPY_MODEL_1696edf8397e4ce8bcfd059b8a793c85"
      ],
      "layout": "IPY_MODEL_fd0aa7ad3bc54373a4b6b70b5fdc147f"
     }
    },
    "3111779021c2433c97372496634a7506": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f786a3dfb6124fe7934f64b2ae5737e0",
      "placeholder": "​",
      "style": "IPY_MODEL_9561ab9105304509b5994af3f5af0e72",
      "value": "Loading checkpoint shards:  20%"
     }
    },
    "3938eb4a901143f297155d1107e388a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b2aac95236b49f6bfd7cd989400cc90",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_87bf1b5b900048378753657fddfee403",
      "value": 1
     }
    },
    "87bf1b5b900048378753657fddfee403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b2aac95236b49f6bfd7cd989400cc90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9561ab9105304509b5994af3f5af0e72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f32abfca153a4c1aacfd15903a40b935": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f786a3dfb6124fe7934f64b2ae5737e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd0aa7ad3bc54373a4b6b70b5fdc147f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
