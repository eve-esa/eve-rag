# RAG Evaluation Configuration
# This file configures the evaluation of a RAG system using a Q&A dataset

# ============================================================================
# INPUT
# ============================================================================
input:
  # Path to Q&A dataset JSON file (generated by generate_qa_dataset.py)
  qa_dataset: "qa_evaluation_dataset.json"

# ============================================================================
# EMBEDDINGS
# ============================================================================
embeddings:
  # Embedding model name
  model: "text-embedding-3-small"

  # API credentials
  api_key: "your-openai-api-key-here"
  base_url: "https://api.openai.com/v1"  # Or your custom endpoint

# ============================================================================
# QDRANT
# ============================================================================
qdrant:
  # Qdrant connection details
  url: "https://your-cluster.cloud.qdrant.io"
  api_key: "your-qdrant-api-key-here"

  # Collection configuration
  collection_name: "sample_collection"

  # Field names in Qdrant payload
  file_path_field: "file_path"
  content_field: "content"

  # Minimum score threshold for retrieval
  score_threshold: 0.3

# ============================================================================
# EVALUATION
# ============================================================================
evaluation:
  # K values to evaluate (number of documents to retrieve)
  # The script will retrieve max(k_values) documents once,
  # then compute metrics at each K value
  k_values: [3, 5, 10, 15, 20]

  # Fuzzy matching threshold for reference validation
  # Fraction of tokens that must match to consider a reference found
  fuzzy_threshold: 0.8

# ============================================================================
# OUTPUT
# ============================================================================
output:
  # Summary file with average metrics at each K value
  summary_file: "evaluation_summary.json"

  # Detailed results file with per-question metrics
  detailed_results_file: "evaluation_detailed_results.json"

  # Whether to save detailed per-question results
  # Set to false to only save summary statistics
  save_detailed_results: true

  # Whether to overwrite existing output files
  overwrite: false

# ============================================================================
# LOGGING
# ============================================================================
logging:
  # Logging level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"

  # Optional: log file path (omit to log to stdout only)
  # log_file: "logs/evaluation.log"