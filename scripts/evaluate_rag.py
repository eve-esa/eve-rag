#!/usr/bin/env python3
"""
RAG Evaluation Script

This script evaluates a RAG system using a Q&A dataset with ground truth references.
It computes token-level, passage-level, and document-level metrics at multiple K values.

Usage:
    python evaluate_rag.py --config config.yaml

Requirements:
    - YAML configuration file with API keys and Qdrant credentials
    - Q&A dataset JSON file (generated by generate_qa_dataset.py)
    - Qdrant collection with document chunks
"""

import argparse
import json
import logging
import os
import sys
import time
from collections import Counter
from typing import List, Dict, Optional
import string

import yaml
from openai import OpenAI
from qdrant_client import QdrantClient
from tqdm import tqdm


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def setup_logging(level: str = "INFO", log_file: Optional[str] = None):
    """Setup logging configuration."""
    log_level = getattr(logging, level.upper(), logging.INFO)

    handlers = [logging.StreamHandler(sys.stdout)]
    if log_file:
        os.makedirs(os.path.dirname(log_file) if os.path.dirname(log_file) else ".", exist_ok=True)
        handlers.append(logging.FileHandler(log_file))

    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=handlers
    )


def load_config(config_path: str) -> Dict:
    """Load YAML configuration file."""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def normalize_text(text: str) -> str:
    """Normalize text by lowercasing, removing punctuation, and normalizing whitespace."""
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    text = " ".join(text.split())
    return text


def is_reference_present_fuzzy(reference: str, document: str, threshold: float = 0.8) -> bool:
    """
    Returns True if enough of the reference tokens appear in the document.

    Args:
        reference: Reference text to find
        document: Document to search in
        threshold: Fraction of tokens that must match (default: 0.8)

    Returns:
        True if the reference is found with sufficient token overlap
    """
    ref_tokens = normalize_text(reference).split()
    doc_tokens = normalize_text(document).split()
    if not ref_tokens:
        return False
    matched_tokens = sum(1 for t in ref_tokens if t in doc_tokens)
    fraction_matched = matched_tokens / len(ref_tokens)
    return fraction_matched >= threshold


# ============================================================================
# EMBEDDING CLIENT
# ============================================================================

class EmbeddingClient:
    """Client for generating embeddings via API."""

    def __init__(self, config: Dict):
        """Initialize embedding client."""
        api_key = config['embeddings']['api_key']
        base_url = config['embeddings'].get('base_url', 'https://api.openai.com/v1')

        if not api_key:
            raise ValueError("Embedding API key not found in config!")

        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = config['embeddings']['model']

        logging.info(f"✓ Embedding client initialized: {self.model_name}")

    def encode(self, text: str) -> List[float]:
        """Generate embedding for a single text."""
        response = self.client.embeddings.create(
            model=self.model_name,
            input=text
        )
        return response.data[0].embedding

    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts."""
        response = self.client.embeddings.create(
            model=self.model_name,
            input=texts
        )
        return [item.embedding for item in response.data]


# ============================================================================
# QDRANT CLIENT
# ============================================================================

class QdrantManager:
    """Manager for Qdrant operations."""

    def __init__(self, config: Dict):
        """Initialize Qdrant client."""
        url = config['qdrant']['url']
        api_key = config['qdrant']['api_key']

        if not url or not api_key:
            raise ValueError("Qdrant credentials not found in config!")

        self.client = QdrantClient(url=url, api_key=api_key)
        self.collection_name = config['qdrant']['collection_name']
        self.file_path_field = config['qdrant']['file_path_field']
        self.content_field = config['qdrant']['content_field']
        self.score_threshold = config['qdrant'].get('score_threshold', 0.3)

        # Verify connection
        info = self.client.get_collection(self.collection_name)
        logging.info(f"✓ Connected to Qdrant collection: {self.collection_name}")
        logging.info(f"  Points: {info.points_count}, Status: {info.status}")

    def search(self, query_embedding: List[float], limit: int = 10) -> List[Dict]:
        """
        Search for similar documents in Qdrant.

        Returns:
            List of dicts with 'content', 'file_path', 'score'
        """
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=query_embedding,
            limit=limit,
            score_threshold=self.score_threshold
        )

        retrieved = []
        for point in results.points:
            content = point.payload.get(self.content_field, '') or point.payload.get('text', '')
            file_path = point.payload.get(self.file_path_field, '')
            retrieved.append({
                'content': content,
                'file_path': file_path,
                'score': point.score
            })

        return retrieved


# ============================================================================
# METRIC COMPUTATION FUNCTIONS
# ============================================================================

def compute_token_metrics(references: List[str], retrieved_texts: List[str], threshold: float = 0.8) -> Dict[str, float]:
    """
    Compute token-level IoU, precision, recall, and F1 score.

    Args:
        references: List of reference texts that should be found
        retrieved_texts: List of retrieved document chunks
        threshold: Threshold for fuzzy matching (default: 0.8)

    Returns:
        Dictionary with 'iou', 'precision', 'recall', 'f1' scores
    """
    all_ref_tokens = []
    all_doc_tokens = []

    # Track which references are found
    for ref in references:
        found = any(is_reference_present_fuzzy(ref, doc, threshold) for doc in retrieved_texts)
        ref_tokens = normalize_text(ref).split()
        all_ref_tokens.extend(ref_tokens)
        if found:
            # Add tokens from retrieved documents
            for doc in retrieved_texts:
                all_doc_tokens.extend(normalize_text(doc).split())

    if not all_ref_tokens:
        return {"iou": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}

    ref_counter = Counter(all_ref_tokens)
    doc_counter = Counter(all_doc_tokens)

    intersection_tokens = ref_counter & doc_counter
    intersection_count = sum(intersection_tokens.values())

    ref_count = sum(ref_counter.values())
    doc_count = sum(doc_counter.values())

    union_count = ref_count + doc_count - intersection_count

    iou = intersection_count / union_count if union_count > 0 else 0.0
    precision = intersection_count / doc_count if doc_count > 0 else 0.0
    recall = intersection_count / ref_count if ref_count > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    return {"iou": iou, "precision": precision, "recall": recall, "f1": f1}


def compute_passage_metrics(references: List[str], retrieved_texts: List[str], threshold: float = 0.8) -> Dict[str, float]:
    """
    Compute passage-level coverage, accuracy, precision, recall, and F1 score.

    Passage-level treats each reference as a unit and checks if it appears in any retrieved chunk.

    Args:
        references: List of reference texts (passages) that should be found
        retrieved_texts: List of retrieved document chunks
        threshold: Threshold for fuzzy matching (default: 0.8)

    Returns:
        Dictionary with 'coverage', 'accuracy', 'precision', 'recall', 'f1' scores
    """
    if not references:
        return {"coverage": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}

    if not retrieved_texts:
        return {"coverage": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}

    # Count how many references were found
    found_references = 0
    for ref in references:
        if any(is_reference_present_fuzzy(ref, doc, threshold) for doc in retrieved_texts):
            found_references += 1

    # Count how many retrieved chunks contain at least one reference
    relevant_retrieved = 0
    for doc in retrieved_texts:
        if any(is_reference_present_fuzzy(ref, doc, threshold) for ref in references):
            relevant_retrieved += 1

    # Coverage: fraction of references found
    coverage = found_references / len(references)

    # Accuracy: 1 if all references found, 0 otherwise
    accuracy = 1.0 if found_references == len(references) else 0.0

    # Precision: fraction of retrieved chunks that contain at least one reference
    precision = relevant_retrieved / len(retrieved_texts) if retrieved_texts else 0.0

    # Recall: fraction of references that were found
    recall = found_references / len(references)

    # F1: harmonic mean of precision and recall
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    return {
        "coverage": coverage,
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }


def compute_document_metrics(source_file_path: str, retrieved_file_paths: List[str]) -> Dict[str, float]:
    """
    Compute document-level coverage, accuracy, precision, and recall.

    Document-level checks if chunks from the source document appear in the retrieved results.

    Args:
        source_file_path: File path of the source document
        retrieved_file_paths: List of file paths of retrieved chunks

    Returns:
        Dictionary with 'coverage', 'accuracy', 'precision', 'recall', 'source_chunks_count' scores
    """
    if not retrieved_file_paths:
        return {"coverage": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "source_chunks_count": 0}

    # Count how many retrieved chunks are from the source document
    source_chunks_retrieved = sum(1 for fp in retrieved_file_paths if fp == source_file_path)

    # Coverage/Accuracy: binary - did we retrieve at least one chunk from source document?
    coverage = 1.0 if source_chunks_retrieved > 0 else 0.0
    accuracy = coverage  # Same as coverage for document-level

    # Precision: fraction of retrieved chunks that are from the source document
    precision = source_chunks_retrieved / len(retrieved_file_paths)

    # Recall: For this metric, we define it as binary (did we find the source doc?)
    recall = coverage

    return {
        "coverage": coverage,
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "source_chunks_count": source_chunks_retrieved
    }


# ============================================================================
# EVALUATOR
# ============================================================================

class RAGEvaluator:
    """Evaluate RAG system performance."""

    def __init__(self, config: Dict, embedding_client: EmbeddingClient, qdrant_manager: QdrantManager):
        """Initialize RAG evaluator."""
        self.config = config
        self.embedder = embedding_client
        self.qdrant = qdrant_manager
        self.threshold = config['evaluation']['fuzzy_threshold']
        self.k_values = config['evaluation']['k_values']
        self.k_max = max(self.k_values)

        # Results storage
        self.results_by_k = {k: [] for k in self.k_values}

    def evaluate_question(self, qa_pair: Dict) -> Dict[str, List[Dict]]:
        """
        Evaluate a single Q&A pair at all K values.

        Returns:
            Dictionary mapping K values to result dictionaries
        """
        question = qa_pair["question"]
        file_path = qa_pair["file_path"]
        references_info = qa_pair["references"]

        # Extract reference texts for evaluation
        reference_texts = [ref_info["text"] for ref_info in references_info]

        # Generate query embedding
        start_time = time.time()
        query_embedding = self.embedder.encode(question)
        embedding_time = time.time() - start_time

        # Retrieve documents from Qdrant (retrieve K_MAX)
        start_time = time.time()
        search_results = self.qdrant.search(query_embedding, limit=self.k_max)
        retrieval_time = time.time() - start_time

        # Extract all retrieved document contents and their file paths
        all_retrieved_texts = [r['content'] for r in search_results]
        all_retrieved_scores = [r['score'] for r in search_results]
        all_retrieved_file_paths = [r['file_path'] for r in search_results]

        # Compute metrics at different K values
        results = {}
        for k in self.k_values:
            # Take only top-K results
            retrieved_texts = all_retrieved_texts[:k]
            retrieved_scores = all_retrieved_scores[:k]
            retrieved_file_paths = all_retrieved_file_paths[:k]

            # Compute token-level metrics
            token_metrics = compute_token_metrics(reference_texts, retrieved_texts, threshold=self.threshold)

            # Compute passage-level metrics
            passage_metrics = compute_passage_metrics(reference_texts, retrieved_texts, threshold=self.threshold)

            # Compute document-level metrics
            document_metrics = compute_document_metrics(file_path, retrieved_file_paths)

            # Store results
            result = {
                "question": question,
                "file_path": file_path,
                "k": k,
                "num_references": len(reference_texts),
                "num_retrieved": len(retrieved_texts),
                "embedding_time": embedding_time,
                "retrieval_time": retrieval_time,
                "avg_score": sum(retrieved_scores) / len(retrieved_scores) if retrieved_scores else 0,

                # Token-level metrics
                "token_iou": token_metrics["iou"],
                "token_precision": token_metrics["precision"],
                "token_recall": token_metrics["recall"],
                "token_f1": token_metrics["f1"],

                # Passage-level metrics
                "passage_coverage": passage_metrics["coverage"],
                "passage_accuracy": passage_metrics["accuracy"],
                "passage_precision": passage_metrics["precision"],
                "passage_recall": passage_metrics["recall"],
                "passage_f1": passage_metrics["f1"],

                # Document-level metrics
                "doc_coverage": document_metrics["coverage"],
                "doc_accuracy": document_metrics["accuracy"],
                "doc_precision": document_metrics["precision"],
                "doc_recall": document_metrics["recall"],
                "doc_chunks_retrieved": document_metrics["source_chunks_count"]
            }

            results[k] = result

        return results

    def evaluate_dataset(self, qa_dataset: List[Dict]):
        """Evaluate entire Q&A dataset."""
        logging.info(f"Evaluating {len(qa_dataset)} Q&A pairs...")
        logging.info(f"Retrieving top-{self.k_max} documents for each question")
        logging.info(f"Computing metrics at K = {self.k_values}")
        logging.info(f"Computing Token-level, Passage-level, and Document-level metrics\n")

        for qa_pair in tqdm(qa_dataset, desc="Evaluating Q&A pairs"):
            try:
                results = self.evaluate_question(qa_pair)

                # Store results for each K value
                for k, result in results.items():
                    self.results_by_k[k].append(result)

            except Exception as e:
                logging.error(f"Error evaluating question '{qa_pair.get('question', 'unknown')}': {e}")
                continue

        logging.info(f"\n✓ Evaluation complete: {len(qa_dataset)} Q&A pairs evaluated at K = {self.k_values}")

    def compute_summary_statistics(self) -> Dict[int, Dict]:
        """Compute average metrics for each K value."""
        summary = {}

        for k in self.k_values:
            results = self.results_by_k[k]

            if not results:
                summary[k] = {}
                continue

            # Compute averages
            n = len(results)
            summary[k] = {
                'k': k,
                'num_questions': n,

                # Token-level
                'token_iou_avg': sum(r['token_iou'] for r in results) / n,
                'token_precision_avg': sum(r['token_precision'] for r in results) / n,
                'token_recall_avg': sum(r['token_recall'] for r in results) / n,
                'token_f1_avg': sum(r['token_f1'] for r in results) / n,

                # Passage-level
                'passage_coverage_avg': sum(r['passage_coverage'] for r in results) / n,
                'passage_accuracy_avg': sum(r['passage_accuracy'] for r in results) / n,
                'passage_precision_avg': sum(r['passage_precision'] for r in results) / n,
                'passage_recall_avg': sum(r['passage_recall'] for r in results) / n,
                'passage_f1_avg': sum(r['passage_f1'] for r in results) / n,

                # Document-level
                'doc_coverage_avg': sum(r['doc_coverage'] for r in results) / n,
                'doc_accuracy_avg': sum(r['doc_accuracy'] for r in results) / n,
                'doc_precision_avg': sum(r['doc_precision'] for r in results) / n,
                'doc_recall_avg': sum(r['doc_recall'] for r in results) / n,
                'doc_chunks_retrieved_avg': sum(r['doc_chunks_retrieved'] for r in results) / n,

                # Timing
                'embedding_time_avg': sum(r['embedding_time'] for r in results) / n,
                'retrieval_time_avg': sum(r['retrieval_time'] for r in results) / n,
                'avg_score_avg': sum(r['avg_score'] for r in results) / n,
            }

        return summary

    def print_summary(self, summary: Dict[int, Dict]):
        """Print summary statistics."""
        logging.info("\n" + "="*80)
        logging.info("AVERAGE EVALUATION METRICS AT DIFFERENT K VALUES")
        logging.info("="*80)

        for k in self.k_values:
            stats = summary[k]

            if not stats:
                logging.info(f"\nK = {k}: No results")
                continue

            logging.info(f"\n{'─'*80}")
            logging.info(f"K = {k}")
            logging.info(f"{'─'*80}")

            # Token-level metrics
            logging.info(f"\n  [TOKEN-LEVEL]")
            logging.info(f"    IoU: {stats['token_iou_avg']:.4f}  |  "
                        f"Precision: {stats['token_precision_avg']:.4f}  |  "
                        f"Recall: {stats['token_recall_avg']:.4f}  |  "
                        f"F1: {stats['token_f1_avg']:.4f}")

            # Passage-level metrics
            logging.info(f"\n  [PASSAGE-LEVEL]")
            logging.info(f"    Coverage: {stats['passage_coverage_avg']:.4f}  |  "
                        f"Accuracy: {stats['passage_accuracy_avg']:.4f}  |  "
                        f"Precision: {stats['passage_precision_avg']:.4f}  |  "
                        f"Recall: {stats['passage_recall_avg']:.4f}  |  "
                        f"F1: {stats['passage_f1_avg']:.4f}")

            # Document-level metrics
            logging.info(f"\n  [DOCUMENT-LEVEL]")
            logging.info(f"    Coverage: {stats['doc_coverage_avg']:.4f}  |  "
                        f"Accuracy: {stats['doc_accuracy_avg']:.4f}  |  "
                        f"Precision: {stats['doc_precision_avg']:.4f}  |  "
                        f"Recall: {stats['doc_recall_avg']:.4f}  |  "
                        f"Avg Chunks: {stats['doc_chunks_retrieved_avg']:.2f}")

        logging.info("\n" + "="*80)


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def load_qa_dataset(file_path: str) -> List[Dict]:
    """Load Q&A dataset from JSON file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    logging.info(f"✓ Loaded {len(dataset)} Q&A pairs from {file_path}")
    return dataset


def save_results(results: Dict[int, List[Dict]], output_file: str, overwrite: bool = False):
    """Save evaluation results to JSON file."""
    # Create output directory if needed
    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)

    # Check if file exists
    if os.path.exists(output_file) and not overwrite:
        logging.error(f"Output file already exists: {output_file}")
        logging.error("Set 'overwrite: true' in config to overwrite")
        return False

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    logging.info(f"✓ Results saved to: {output_file}")
    return True


def save_summary(summary: Dict[int, Dict], output_file: str, overwrite: bool = False):
    """Save summary statistics to JSON file."""
    # Create output directory if needed
    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)

    # Check if file exists
    if os.path.exists(output_file) and not overwrite:
        logging.error(f"Output file already exists: {output_file}")
        logging.error("Set 'overwrite: true' in config to overwrite")
        return False

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    logging.info(f"✓ Summary saved to: {output_file}")
    return True


def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(
        description="Evaluate RAG system using Q&A dataset",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example:
    python evaluate_rag.py --config config.yaml

Make sure to configure the following in your config.yaml:
    - Embedding API credentials (api_key, base_url, model)
    - Qdrant credentials (url, api_key, collection_name)
    - Evaluation parameters (k_values, fuzzy_threshold)
        """
    )
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='Path to YAML configuration file'
    )

    args = parser.parse_args()

    # Load configuration
    config = load_config(args.config)

    # Setup logging
    setup_logging(
        level=config['logging']['level'],
        log_file=config['logging'].get('log_file')
    )

    logging.info("="*80)
    logging.info("RAG SYSTEM EVALUATION")
    logging.info("="*80)
    logging.info(f"Configuration: {args.config}")

    # Initialize clients
    embedding_client = EmbeddingClient(config)
    qdrant_manager = QdrantManager(config)

    # Initialize evaluator
    evaluator = RAGEvaluator(config, embedding_client, qdrant_manager)

    # Load Q&A dataset
    logging.info(f"\nLoading Q&A dataset from: {config['input']['qa_dataset']}")
    qa_dataset = load_qa_dataset(config['input']['qa_dataset'])

    # Run evaluation
    logging.info(f"\nStarting evaluation...")
    evaluator.evaluate_dataset(qa_dataset)

    # Compute summary statistics
    summary = evaluator.compute_summary_statistics()

    # Print summary
    evaluator.print_summary(summary)

    # Save detailed results
    if config['output']['save_detailed_results']:
        success = save_results(
            evaluator.results_by_k,
            config['output']['detailed_results_file'],
            overwrite=config['output'].get('overwrite', False)
        )

        if not success:
            logging.error("Failed to save detailed results")
            return 1

    # Save summary
    success = save_summary(
        summary,
        config['output']['summary_file'],
        overwrite=config['output'].get('overwrite', False)
    )

    if not success:
        logging.error("Failed to save summary")
        return 1

    logging.info(f"\n✓ Evaluation completed successfully!")
    logging.info(f"  Summary: {config['output']['summary_file']}")
    if config['output']['save_detailed_results']:
        logging.info(f"  Detailed results: {config['output']['detailed_results_file']}")

    return 0


if __name__ == "__main__":
    sys.exit(main())