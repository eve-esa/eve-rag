{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Query Rewriting for Multi-Turn Conversations\n",
    "\n",
    "In this notebook, we demonstrate how **query rewriting** is essential for effective retrieval in multi-turn conversational scenarios.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "In multi-turn conversations, users often:\n",
    "- Use pronouns (\"it\", \"they\", \"that\")\n",
    "- Make implicit references to previous topics\n",
    "- Ask follow-up questions without full context\n",
    "- Use abbreviations that need expansion\n",
    "\n",
    "These natural conversation patterns make it difficult for RAG systems to retrieve relevant documents because the query lacks context.\n",
    "\n",
    "## The Solution\n",
    "\n",
    "Query rewriting transforms context-dependent queries into self-contained versions by:\n",
    "1. Incorporating conversation history\n",
    "2. Replacing pronouns with specific referents\n",
    "3. Expanding abbreviations and acronyms\n",
    "4. Extracting core content while removing conversational fluff\n",
    "\n",
    "We'll demonstrate this by comparing retrieval performance with and without query rewriting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Qdrant connection\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# Validate credentials\n",
    "if not QDRANT_URL or not QDRANT_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"Qdrant credentials not found!\\n\"\n",
    "        \"Please set QDRANT_URL and QDRANT_API_KEY in your .env file.\"\n",
    "    )\n",
    "\n",
    "collection_name = \"sample_collection.snapshot\"\n",
    "\n",
    "print(f\"✓ Environment configured\")\n",
    "print(f\"  Qdrant URL: {QDRANT_URL}\")\n",
    "print(f\"  Collection: {collection_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "id": "connect-qdrant",
   "metadata": {},
   "source": [
    "# Connect to Qdrant\n",
    "try:\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "    # Verify connection\n",
    "    info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"✓ Connected to Qdrant\")\n",
    "    print(f\"  Collection: {collection_name}\")\n",
    "    print(f\"  Points: {info.points_count}\")\n",
    "    print(f\"  Status: {info.status}\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to connect to Qdrant: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "embedder-header",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "init-embedder",
   "metadata": {},
   "source": [
    "# Initialize the embedding model\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"✓ Embedding model loaded: all-MiniLM-L6-v2\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "llm-header",
   "metadata": {},
   "source": "## Initialize Language Model for Query Rewriting\n\nYou have two options for the language model:\n\n**Option 1: Local Model (No API required)**\n- Uses Hugging Face Transformers to run a model locally\n- Recommended: `microsoft/Phi-3.5-mini-instruct` (~7GB download)\n- Pros: No API costs, works offline, full privacy\n- Cons: Requires GPU/CPU resources, slower than API\n\n**Option 2: API-based Model (Requires API key)**\n- Uses OpenAI API or compatible providers (Ollama, LocalAI, vLLM, etc.)\n- Recommended: `gpt-4o-mini` for cost-effectiveness\n- Pros: Fast, no local resources needed\n- Cons: Requires API key, costs per token\n\nChoose the option that best fits your setup by uncommenting the appropriate section below."
  },
  {
   "cell_type": "code",
   "id": "init-llm",
   "metadata": {},
   "source": "import torch\nfrom transformers import pipeline\n\n# ============================================================================\n# OPTION 1: Use a local model (loads model weights locally)\n# ============================================================================\n# Uncomment the lines below to use a local model\n# This is useful if you don't have an API key or want to run everything locally\n# Note: First time will download ~7GB of model weights\n\n# model_id = \"microsoft/Phi-3.5-mini-instruct\"\n# model = pipeline(\n#     \"text-generation\",\n#     model=model_id,\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"auto\",\n# )\n# \n# def call_llm(messages: List[Dict], max_tokens: int = 500, temperature: float = 0.1) -> str:\n#     \"\"\"\n#     Call the local LLM with given messages.\n#     \n#     Args:\n#         messages: List of message dicts with 'role' and 'content'\n#         max_tokens: Maximum tokens to generate\n#         temperature: Sampling temperature\n#     \n#     Returns:\n#         Generated text response\n#     \"\"\"\n#     response = model(messages, max_new_tokens=max_tokens, temperature=temperature)\n#     return response[0]['generated_text'][-1]['content']\n# \n# print(f\"✓ Local LLM initialized: {model_id}\")\n\n# ============================================================================\n# OPTION 2: Use an API-based model (requires API key)\n# ============================================================================\n# Comment out this section if using OPTION 1 (local model)\n# This works with OpenAI and any OpenAI-compatible provider (LocalAI, Ollama, vLLM, etc.)\n\nfrom openai import OpenAI\n\n# Initialize OpenAI client (or compatible provider)\nclient = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    base_url=os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")  # Defaults to OpenAI if not set\n)\n\ndef call_llm(messages: List[Dict], max_tokens: int = 500, temperature: float = 0.1) -> str:\n    \"\"\"\n    Call the LLM API with given messages.\n    \n    Args:\n        messages: List of message dicts with 'role' and 'content'\n        max_tokens: Maximum tokens to generate\n        temperature: Sampling temperature\n    \n    Returns:\n        Generated text response\n    \"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",  # or \"gpt-4\", \"gpt-3.5-turbo\", or local model name for compatible providers\n        messages=messages,\n        max_tokens=max_tokens,\n        temperature=temperature\n    )\n    return response.choices[0].message.content\n\nprint(f\"✓ LLM initialized: gpt-4o-mini (API)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "prompt-header",
   "metadata": {},
   "source": [
    "## Query Rewriting Prompt\n",
    "\n",
    "This prompt instructs the LLM to rewrite queries by:\n",
    "- Incorporating conversation context\n",
    "- Replacing pronouns with specific referents\n",
    "- Expanding abbreviations and acronyms (context-aware)\n",
    "- Removing conversational fluff\n",
    "- Extracting core content"
   ]
  },
  {
   "cell_type": "code",
   "id": "rewriting-prompt",
   "metadata": {},
   "source": [
    "query_rewriting_prompt = \"\"\"You are an expert assistant specializing in Earth Observation (EO). Your task is to rewrite the user's final query into a self-contained version suitable for a RAG (Retrieval-Augmented Generation) system.\n",
    "\n",
    "Your rewritten query must:\n",
    "- Preserve the user's original intent and core question\n",
    "- Include all necessary context from the conversation history\n",
    "- Be interpretable without access to prior conversation turns\n",
    "- Disambiguate towards Earth Observation concepts when ambiguous\n",
    "- Expand abbreviations and acronyms for clarity when needed\n",
    "- Be concise and focused on the core content\n",
    "\n",
    "Rewriting Rules:\n",
    "\n",
    "1. Context incorporation:\n",
    "   - If the query references previous context (pronouns, implicit topics, follow-ups), incorporate necessary context\n",
    "   - Replace pronouns (it, they, this, that) with their specific referents\n",
    "   - Make implicit references explicit\n",
    "\n",
    "2. Core Content Extraction:\n",
    "   - Reduce the amount of information in the original question\n",
    "   - Extract only the most core content needed to retrieve relevant information\n",
    "   - Remove conversational fluff, politeness phrases, and redundant information\n",
    "   - The rewritten query should be brief and focused\n",
    "   - Keep it shorter than simply replacing keywords - aim for concise, essential information only\n",
    "\n",
    "3. Clarification and expansion:\n",
    "   - Expand abbreviations and acronyms (e.g., \"EO\" → \"Earth Observation\", \"ESA\" → \"European Space Agency\", \"SAR\" → \"Synthetic Aperture Radar\", \"NDVI\" → \"Normalized Difference Vegetation Index\")\n",
    "   - EXCEPTION: Do NOT expand acronyms if the conversation context already clearly establishes what they refer to\n",
    "   - When ambiguous terms could refer to EO or non-EO concepts, disambiguate towards Earth Observation\n",
    "   - Maintain technical accuracy\n",
    "\n",
    "4. Context-aware acronym handling:\n",
    "   - If previous conversation turns have already mentioned and explained an acronym, keep it as-is in the rewritten query\n",
    "   - If the acronym appears for the first time or context is unclear, expand it\n",
    "   - Common well-established acronyms in Earth Observation (e.g., \"SAR\", \"NDVI\", \"RGB\") can remain unexpanded if they appear in a clearly technical EO context\n",
    "\n",
    "5. What NOT to do:\n",
    "   - Do NOT add information not present in the conversation\n",
    "   - Do NOT change technical terms unnecessarily\n",
    "   - Do NOT keep conversational elements like \"I was wondering\", \"Could you please\", \"Thank you\"\n",
    "   - Do NOT preserve unnecessary details that don't affect the core query\n",
    "   - Do NOT rewrite if the user explicitly asks for their original query to be kept\n",
    "\n",
    "Examples:\n",
    "\n",
    "Example 1 (Abbreviation expansion + core extraction):\n",
    "Conversation: [empty]\n",
    "Last query: \"Hi! I was wondering if you could tell me what are the main applications of S2 data? Thanks!\"\n",
    "→ rewritten_query: \"Sentinel-2 data application\"\n",
    "\n",
    "Example 2 (Acronym expansion + core extraction):\n",
    "Conversation: [empty]\n",
    "Last query: \"Could you please explain to me how SAR technology works in remote sensing?\"\n",
    "→ rewritten_query: \"Synthetic Aperture Radar remote sensing\"\n",
    "\n",
    "Example 3 (Context incorporation + core extraction):\n",
    "Conversation:\n",
    "User: \"Tell me about Sentinel-1\"\n",
    "Assistant: \"Sentinel-1 is a radar imaging mission...\"\n",
    "Last query: \"That's interesting! What is its spatial resolution and how does it compare to other satellites?\"\n",
    "→ rewritten_query: \"Sentinel-1 spatial resolution comparison\"\n",
    "\n",
    "Example 4 (Implicit reference + core extraction):\n",
    "Conversation:\n",
    "User: \"What sensors does Landsat 8 have?\"\n",
    "Assistant: \"Landsat 8 carries OLI and TIRS sensors...\"\n",
    "Last query: \"Which of those bands would be best for vegetation monitoring in agricultural areas?\"\n",
    "→ rewritten_query: \"Vegetation monitoring agricultural areas Landsat 8 bands\"\n",
    "\n",
    "Example 5 (Ambiguity resolution + core extraction):\n",
    "Conversation: [empty]\n",
    "Last query: \"I'm trying to understand what are the different types of resolution that exist?\"\n",
    "→ rewritten_query: \"types of resolution in Earth Observation\"\n",
    "\n",
    "Now process the following:\n",
    "\n",
    "Conversation:\n",
    "{conversation}\n",
    "\n",
    "Last query:\n",
    "{last_utterance}\n",
    "\n",
    "Respond with ONLY the rewritten query, nothing else.\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "load-conversations-header",
   "metadata": {},
   "source": [
    "## Load Conversation Examples\n",
    "\n",
    "We load real multi-turn conversation examples to demonstrate query rewriting."
   ]
  },
  {
   "cell_type": "code",
   "id": "load-conversations",
   "metadata": {},
   "source": [
    "# Load conversations from JSONL file\n",
    "conversations = []\n",
    "with open('../data/eo_conversations.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "print(f\"✓ Loaded {len(conversations)} conversation examples\")\n",
    "print(f\"\\nExample conversation:\")\n",
    "print(f\"{'='*80}\")\n",
    "sample = conversations[2]  # Example with context\n",
    "print(f\"Conversation:\\n{sample['conversation']}\")\n",
    "print(f\"\\nLast utterance: {sample['last_utterance']}\")\n",
    "print(f\"{'='*80}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "rewriting-function-header",
   "metadata": {},
   "source": [
    "## Query Rewriting Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "rewriting-function",
   "metadata": {},
   "source": [
    "def rewrite_query(conversation: str, last_utterance: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite a query using conversation context.\n",
    "    \n",
    "    Args:\n",
    "        conversation: Previous conversation history\n",
    "        last_utterance: The user's last query\n",
    "    \n",
    "    Returns:\n",
    "        Rewritten query\n",
    "    \"\"\"\n",
    "    # Format the prompt\n",
    "    prompt = query_rewriting_prompt.format(\n",
    "        conversation=conversation if conversation else \"[empty]\",\n",
    "        last_utterance=last_utterance\n",
    "    )\n",
    "    \n",
    "    # Call LLM\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    rewritten = call_llm(messages, max_tokens=100, temperature=0.1)\n",
    "    \n",
    "    return rewritten.strip()\n",
    "\n",
    "print(\"✓ Query rewriting function defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-function-header",
   "metadata": {},
   "source": [
    "## Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "retrieval-function",
   "metadata": {},
   "source": [
    "def retrieve_documents(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from Qdrant for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of retrieved documents with content and metadata\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedder.encode([query])[0].tolist()\n",
    "    \n",
    "    # Query Qdrant\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding,\n",
    "        limit=k,\n",
    "        score_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    documents = []\n",
    "    for point in results.points:\n",
    "        documents.append({\n",
    "            'content': point.payload.get('content', '') or point.payload.get('text', ''),\n",
    "            'file_path': point.payload.get('file_path', ''),\n",
    "            'score': point.score\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "print(\"✓ Retrieval function defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-header",
   "metadata": {},
   "source": [
    "## Demonstration: Query Rewriting Examples\n",
    "\n",
    "Let's see how query rewriting transforms context-dependent queries into self-contained ones."
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-rewriting",
   "metadata": {},
   "source": [
    "# Select interesting examples to demonstrate\n",
    "demo_indices = [0, 2, 3, 7, 10, 13]  # Mix of different query types\n",
    "\n",
    "print(\"QUERY REWRITING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rewriting_examples = []\n",
    "\n",
    "for idx in demo_indices:\n",
    "    conv = conversations[idx]\n",
    "    \n",
    "    # Rewrite the query\n",
    "    rewritten = rewrite_query(conv['conversation'], conv['last_utterance'])\n",
    "    \n",
    "    rewriting_examples.append({\n",
    "        'conversation': conv['conversation'],\n",
    "        'original_query': conv['last_utterance'],\n",
    "        'rewritten_query': rewritten\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nExample {len(rewriting_examples)}:\")\n",
    "    print(\"-\" * 80)\n",
    "    if conv['conversation']:\n",
    "        print(f\"Context: {conv['conversation'][:150]}...\")\n",
    "    else:\n",
    "        print(\"Context: [empty]\")\n",
    "    print(f\"\\nOriginal: {conv['last_utterance']}\")\n",
    "    print(f\"Rewritten: {rewritten}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Comparison: Retrieval With vs Without Query Rewriting\n",
    "\n",
    "Now we'll demonstrate the impact of query rewriting on retrieval quality by:\n",
    "1. Retrieving documents using the **original query** (as-is)\n",
    "2. Retrieving documents using the **rewritten query**\n",
    "3. Comparing the results\n",
    "\n",
    "For conversations with context (multi-turn), we expect the rewritten query to perform better."
   ]
  },
  {
   "cell_type": "code",
   "id": "comparison",
   "metadata": {},
   "source": [
    "# Select examples with conversation context (multi-turn scenarios)\n",
    "multi_turn_examples = [conv for conv in conversations if conv['conversation']]\n",
    "\n",
    "print(f\"Comparing retrieval for {len(multi_turn_examples)} multi-turn conversations\")\n",
    "print(f\"Retrieving top-5 documents for each query\\n\")\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for idx, conv in enumerate(multi_turn_examples[:5]):  # Analyze first 5 multi-turn examples\n",
    "    original_query = conv['last_utterance']\n",
    "    \n",
    "    # Rewrite the query\n",
    "    rewritten_query = rewrite_query(conv['conversation'], original_query)\n",
    "    \n",
    "    # Retrieve with original query\n",
    "    original_results = retrieve_documents(original_query, k=5)\n",
    "    \n",
    "    # Retrieve with rewritten query\n",
    "    rewritten_results = retrieve_documents(rewritten_query, k=5)\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results.append({\n",
    "        'conversation': conv['conversation'],\n",
    "        'original_query': original_query,\n",
    "        'rewritten_query': rewritten_query,\n",
    "        'original_docs': original_results,\n",
    "        'rewritten_docs': rewritten_results,\n",
    "        'original_avg_score': sum(d['score'] for d in original_results) / len(original_results) if original_results else 0,\n",
    "        'rewritten_avg_score': sum(d['score'] for d in rewritten_results) / len(rewritten_results) if rewritten_results else 0\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nConversation Context:\")\n",
    "    print(conv['conversation'])\n",
    "    print(f\"\\nOriginal Query: {original_query}\")\n",
    "    print(f\"Rewritten Query: {rewritten_query}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(\"RETRIEVAL WITH ORIGINAL QUERY\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"Average Score: {comparison_results[-1]['original_avg_score']:.4f}\")\n",
    "    print(f\"\\nTop 3 Documents:\")\n",
    "    for i, doc in enumerate(original_results[:3]):\n",
    "        print(f\"\\n  [{i+1}] Score: {doc['score']:.4f}\")\n",
    "        print(f\"      File: {doc['file_path'].split('/')[-1] if doc['file_path'] else 'N/A'}\")\n",
    "        print(f\"      Content: {doc['content'][:150]}...\")\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(\"RETRIEVAL WITH REWRITTEN QUERY\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"Average Score: {comparison_results[-1]['rewritten_avg_score']:.4f}\")\n",
    "    print(f\"\\nTop 3 Documents:\")\n",
    "    for i, doc in enumerate(rewritten_results[:3]):\n",
    "        print(f\"\\n  [{i+1}] Score: {doc['score']:.4f}\")\n",
    "        print(f\"      File: {doc['file_path'].split('/')[-1] if doc['file_path'] else 'N/A'}\")\n",
    "        print(f\"      Content: {doc['content'][:150]}...\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"COMPARISON COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated that **query rewriting is fundamental for multi-turn conversational RAG systems** because:\n",
    "\n",
    "1. **Context Integration**: Rewritten queries incorporate conversation history, making them self-contained\n",
    "2. **Improved Retrieval**: On average, rewritten queries achieve higher retrieval scores\n",
    "3. **Better Relevance**: Retrieved documents are more relevant to the user's actual intent\n",
    "4. **Reduced Ambiguity**: Pronouns and implicit references are resolved\n",
    "5. **Expanded Coverage**: Abbreviations and acronyms are expanded for better matching\n",
    "\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Always use query rewriting** for multi-turn conversational systems\n",
    "2. **Maintain conversation history** to provide context for rewriting\n",
    "3. **Fine-tune the rewriting prompt** for your specific domain\n",
    "4. **Monitor rewriting quality** and adjust the prompt as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
