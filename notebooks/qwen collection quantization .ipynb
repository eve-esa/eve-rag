{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbdb7d2-dd6e-455d-a384-361f72c8391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install qdrant-client\n",
    "!pip install tqdm\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd2fb7-3035-4722-9493-bddf3c6f3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import VectorParams, Distance\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import VectorParams, Distance, HnswConfigDiff, OptimizersConfigDiff,PointStruct\n",
    "import os\n",
    "\n",
    "QDRANT_API_KEY=os.getenv('QDRANT_API_KEY')\n",
    "QDRANT_URL=os.getenv('QDRANT_URL')\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "QDRANT_API_KEY_1=os.getenv('QDRANT_API_KEY_1')\n",
    "QDRANT_URL_1=os.getenv('QDRANT_URL_1')\n",
    "\n",
    "new_client = QdrantClient(\n",
    "    url=QDRANT_URL_1,\n",
    "    api_key=QDRANT_API_KEY_1,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "source_collection = \"esa-data-indus-512-1024-recursive\"\n",
    "quantized_collection = \"esa-data-qwen-1024-recursive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c40bfe-e3d2-42b8-a0ef-628a872bff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=quantized_collection,\n",
    "    vectors_config=models.VectorParams(size=2560, distance=models.Distance.COSINE,on_disk=True),\n",
    "    shard_number=8,  # Increase shards for large data\n",
    "    on_disk_payload=True,\n",
    "    quantization_config=models.BinaryQuantization(\n",
    "        binary=models.BinaryQuantizationConfig(\n",
    "       #     encoding=models.BinaryQuantizationEncoding.TWO_BITS,\n",
    "            always_ram=False,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a8df6a1-feff-4edc-849d-30e44a61da54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.update_collection(\n",
    "    collection_name=quantized_collection,\n",
    "    hnsw_config=models.HnswConfigDiff(\n",
    "        m=16,\n",
    "        ef_construct=128,\n",
    "        full_scan_threshold=10_000,\n",
    "        max_indexing_threads=2,\n",
    "        on_disk=True\n",
    "    ),\n",
    "    optimizers_config=models.OptimizersConfigDiff(\n",
    "            indexing_threshold=20000,          # start indexing after 20k vectors per segment\n",
    "            memmap_threshold=5000,             # smaller than indexing_threshold; helps with RAM limits\n",
    "            deleted_threshold=0.2,             # when >20% deleted, trigger segment cleanup\n",
    "            vacuum_min_vector_number=1000,     # minimum segment size for vacuuming\n",
    "            default_segment_number=4,          # spread data across 4 segments instead of 2\n",
    "            max_segment_size=6_000_000,       # keep segments smaller (avoid huge merges)\n",
    "            max_optimization_threads=1,        # limit parallel merges (less memory/disk pressure)\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1abf3c-0a0e-4a69-aa37-809f47d0e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2  # seconds\n",
    "\n",
    "\n",
    "\n",
    "class qwen_embedder:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen3-Embedding-4B\"):\n",
    "        # Load the sentence-transformers model\n",
    "        self.model = SentenceTransformer(\n",
    "                                    model_name,\n",
    "                                    model_kwargs={\n",
    "                                        \"dtype\": \"auto\",       # important: will use float16/bfloat16 automatically\n",
    "                                        \"device_map\": \"auto\",\n",
    "                                    },\n",
    "                                    tokenizer_kwargs={\"padding_side\": \"left\",\n",
    "                                                      \"max_length\": 2048,\n",
    "                                                      \"truncation\": True                                                      \n",
    "                                                      }\n",
    "                                                      )\n",
    "\n",
    "    def embed_documents(self, \n",
    "                        texts,\n",
    "                        batch_size=4, \n",
    "                        padding=True, \n",
    "                        truncation=True, \n",
    "                        max_length=2048, \n",
    "                        normalize=True):\n",
    "        \"\"\"\n",
    "        Encodes a list of texts into embeddings.\n",
    "\n",
    "        Args:\n",
    "            texts (list[str]): Documents to embed\n",
    "            padding (bool/str): True = dynamic padding, 'max_length' = fixed length\n",
    "            truncation (bool): Whether to truncate texts beyond max_length\n",
    "            max_length (int): Max tokens allowed\n",
    "            normalize (bool): Whether to L2 normalize embeddings\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Embeddings array (num_texts x embedding_dim)\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size, \n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            normalize_embeddings=normalize,\n",
    "            convert_to_numpy=True,\n",
    "            convert_to_tensor=False \n",
    "        )\n",
    "        embeddings = embeddings.tolist()\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "    def embed_query(self,query):\n",
    "            \n",
    "        embeddings = self.model.encode( query,prompt_name=\"query\")\n",
    "\n",
    "        embeddings = embeddings.tolist()\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dca342-5502-4605-a497-57a0e0242f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29d7789bce64ca9ba807f69da69a4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb=qwen_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f13ee-c36b-4338-bb7e-cb1053a5b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping points: 100%|██████████| 20000/20000 [00:00<00:00, 23630.52pts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target at offset: 154619806622784387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#Get the offset id if collection migration stopped in middle\n",
    "SCROLL_BATCH = 1000 # increase batch size to reduce API calls\n",
    "TARGET = 20000\n",
    "offset = None\n",
    "processed = 0\n",
    "\n",
    "with tqdm(total=TARGET, desc=\"Skipping points\", unit=\"pts\") as pbar:\n",
    "    while True:\n",
    "        scroll_result, next_offset = new_client.scroll(\n",
    "            collection_name=source_collection,\n",
    "            limit=SCROLL_BATCH,\n",
    "            offset=offset,\n",
    "            with_vectors=False,\n",
    "            with_payload=False  # fastest: no vectors, no payloads\n",
    "        )\n",
    "\n",
    "        if not scroll_result:\n",
    "            print(\"Reached end of collection before target\")\n",
    "            break\n",
    "\n",
    "        batch_count = len(scroll_result)\n",
    "        processed += batch_count\n",
    "        pbar.update(batch_count)\n",
    "\n",
    "        if processed >= TARGET:\n",
    "            print(f\"Reached target at offset: {next_offset}\")\n",
    "            break\n",
    "\n",
    "        offset = next_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5860146-ba0e-4770-be84-5f24c2b7cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCROLL_BATCH = 2000\n",
    "UPLOAD_BATCH = 1000\n",
    "MAX_WORKERS = 2\n",
    "TOTAL_POINTS = 1377173\n",
    "processed=0\n",
    "offset = None # copy the offset from above to put here to continue migration if first time keep None \n",
    "\n",
    "def upload_batch(points):\n",
    "    if points:\n",
    "        client.upload_points(collection_name=quantized_collection, points=points)\n",
    "    return len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca550c2-f282-4326-b903-6598b32c23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2  # seconds\n",
    "\n",
    "def retry_qdrant(func, *args, **kwargs):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except (UnexpectedResponse, ConnectionError, TimeoutError) as e:\n",
    "            print(f\"Attempt {attempt} failed: {e}\")\n",
    "            if attempt < MAX_RETRIES:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def embed_points_batch(points):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a batch of points using the 'content' field.\n",
    "    Returns a list of PointStruct with vectors.\n",
    "    \"\"\"\n",
    "    contents = []\n",
    "    point_ids = []\n",
    "    payloads = []\n",
    "\n",
    "    for p in points:\n",
    "        content = p.payload.get(\"content\")\n",
    "        if content is None:\n",
    "            raise ValueError(f\"Point {p.id} has no 'content' field\")\n",
    "        contents.append(content)\n",
    "        point_ids.append(p.id)\n",
    "        payloads.append(p.payload)\n",
    "\n",
    "    # Generate embeddings for the entire batch\n",
    "    vectors = emb.embed_documents(contents)\n",
    "\n",
    "    # Build PointStructs with new vectors\n",
    "    embedded_points = [\n",
    "        PointStruct(id=pid, vector=vec, payload=pl)\n",
    "        for pid, vec, pl in zip(point_ids, vectors, payloads)\n",
    "    ]\n",
    "    return embedded_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1449c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=TOTAL_POINTS-processed, desc=\"Migrating points\", unit=\"pts\") as pbar:\n",
    "    while processed < TOTAL_POINTS:\n",
    "        # Scroll points without fetching vectors from Qdrant\n",
    "        scroll_result, next_offset = retry_qdrant(\n",
    "            new_client.scroll,\n",
    "            collection_name=source_collection,\n",
    "            limit=SCROLL_BATCH,\n",
    "            offset=offset,\n",
    "            with_vectors=False  # important\n",
    "        )\n",
    "\n",
    "        if not scroll_result:\n",
    "            break\n",
    "\n",
    "        # Embed points in batch\n",
    "        points = embed_points_batch(scroll_result)\n",
    "\n",
    "        # Split into batches for parallel upload\n",
    "        batches = [points[i:i+UPLOAD_BATCH] for i in range(0, len(points), UPLOAD_BATCH)]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = [executor.submit(retry_qdrant, upload_batch, b) for b in batches]\n",
    "            for f in as_completed(futures):\n",
    "                uploaded = f.result()\n",
    "                pbar.update(uploaded)\n",
    "\n",
    "        if next_offset is None:\n",
    "            break\n",
    "        offset = next_offset\n",
    "\n",
    "        processed+=SCROLL_BATCH\n",
    "\n",
    "print(\"Collection migrated with new embeddings from 'content' field\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
