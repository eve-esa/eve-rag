{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8746653",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "**Ingestion** is the next crucial steps after chunking of data. This is the process of ingesting data from a source and indexing it. The indexing process is composed of three steps:\n",
    "- **Load**: process and load data in text format.\n",
    "- **Split**: this is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "- **Store**: we need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) model.\n",
    "Once the Indexing step is done we will have our knowledge base made of scientific papers indexed and ready to be used in the generation steps as context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf2e4d",
   "metadata": {},
   "source": [
    "## QDrant Vector Database\n",
    "\n",
    "In our case, we will be using the **qdrant** vector database to store our embeddings. You can sign up for a free cluster [here](https://qdrant.tech/documentation/cloud/create-cluster/). In the free tier, you get access to a 1GB cluster, forever free with no credit card signup. If unused, free tier clusters are automatically suspended after 1 week, and deleted after 4 weeks of inactivity if not reactivated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b539129",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "An embeddings model in Retrieval-Augmented Generation (RAG) is a neural network that converts text into dense vector representations (embeddings) in a **high-dimensional space**. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\n",
    "\n",
    "Embeddings models are trained on large text corpora using unsupervised learning techniques. They learn to encode the semantic meaning of words, sentences, and documents in a way that captures relationships between them. For example, embeddings models can learn that \"cat\" and \"dog\" are similar because they are both animals, or that \"apple\" and \"orange\" are similar because they are both fruits.\n",
    "\n",
    "There are many pre-trained embedding models available, each suited to different types of data and use cases.\n",
    "\n",
    "Choosing the right embedding model is a critical step in building an effective retrieval system. Ideally, the embedding model should be trained—or at least fine-tuned—on data similar to the target documents. Since our corpus consists of scientific texts focused on Earth Observation, Indus is a better fit than a general-purpose model, as it captures domain-specific terminology and semantics more accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da0fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the embeddings model\n",
    "model_name = \"nasa-impact/nasa-smd-ibm-st-v2\"\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,  encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e3879f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "result = embedder.embed_query(\"hello\")\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4c137",
   "metadata": {},
   "source": [
    "### Vector Store\n",
    "\n",
    "Vector stores are specialized databases designed to efficiently index and retrieve information using vector representations of data. Vector stores leverages the dense representation by reducing the task of finding similar documents to a search in a high-dimensional space. This search is made by comparing the vector representation of the **query** with the vector representation of the **documents** in the database. The documents that are closer to the query vector are considered more similar to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d81164",
   "metadata": {},
   "source": [
    "Let's see how to create a qdrant collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58186918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, HnswConfigDiff, OptimizersConfigDiff,PointStruct\n",
    "from qdrant_client import models\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# get your keys from the qdrant UI\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "\n",
    "collection_name = \"ingestion_demo\"\n",
    "vector_size = 768 \n",
    "\n",
    "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "    size=vector_size,\n",
    "    distance=Distance.COSINE,\n",
    "    on_disk=True,\n",
    "        ),\n",
    "    shard_number=8,  # increase shards for large data\n",
    "    on_disk_payload=True,\n",
    "    quantization_config=models.BinaryQuantization( # using binary quantization for faster retrieval\n",
    "    binary=models.BinaryQuantizationConfig(\n",
    "    always_ram=False,\n",
    "    ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d78c9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "chunk_1 = \"\"\"Sentinel-2 is a European mission that utilizes wide-swath, high-resolution, multi-spectral imaging. Dedicated to Europe’s Copernicus programme, \n",
    "            the mission supports operational applications primarily for land services, including the monitoring of vegetation, soil and water cover, \n",
    "            as well as the observation of inland waterways and coastal areas.\"\"\"\n",
    "chunk_2 = \"\"\"TROPOMI (Tropospheric Monitoring Instrument) is a cutting-edge satellite instrument aboard the European Copernicus Sentinel-5 Precursor (S5P) satellite, launched in October 2017.\" \n",
    "            It plays an essential role in gathering data that helps scientists to better understand atmospheric processes and environmental changes. \n",
    "            TROPOMI’s high-resolution data contributes to global efforts in monitoring air quality, tracking climate trends, and protecting the ozone layer, \n",
    "            making it a key tool for advancing environmental science and policy worldwide.\"\"\"\n",
    "chunk_3 = \"\"\"Copernicus is the European Union's flagship Earth observation initiative, designed to provide valuable information services to citizens and organizations across the EU. \n",
    "            It utilizes a combination of satellite Earth observation and in-situ (non-space) data to monitor and understand our planet and its environment.\"\"\"\n",
    "chunks = [chunk_1, chunk_2, chunk_3]\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ffaee",
   "metadata": {},
   "source": [
    "We can now batch them together and upload them to the qdrant collection we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a25f3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectors = embedder.embed_documents(chunks)\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=i, \n",
    "        vector=vec,\n",
    "        payload={\"content\" : chunks[i]}\n",
    "    )\n",
    "    for i, vec in enumerate(batch_vectors)\n",
    "]\n",
    "\n",
    "client.upload_points(collection_name=collection_name,\n",
    "                            points=points,\n",
    "                            parallel=10,\n",
    "                            max_retries=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5879e4b",
   "metadata": {},
   "source": [
    "Perfect! In this next notebook, we will see how to retrieve the most relevant chunks for a given user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3190d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eve-rag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
