{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "In this notebook, we will evaluate the performance of our Retrieval-Augmented Generation (RAG) system using the Qdrant vector database populated with Earth Observation documents."
   ],
   "id": "2c7c79e1d7b0efc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup Qdrant Vector Database\n",
    "\n",
    "Before proceeding, you'll need to:\n",
    "\n",
    "1. **Create a Qdrant account**: Sign up for a free account at [Qdrant Cloud](https://cloud.qdrant.io/)\n",
    "2. **Create a cluster**: Follow the setup wizard to create a new Qdrant cluster\n",
    "3. **Get your credentials**:\n",
    "   - **Cluster URL**: Found in your cluster dashboard (e.g., `https://xyz-example.eu-central.aws.cloud.qdrant.io`)\n",
    "   - **API Key**: Generate an API key from the cluster settings\n",
    "4. **Configure your environment**:\n",
    "   - Copy `.env.example` to `.env` (if you haven't already)\n",
    "   - Update `QDRANT_URL` and `QDRANT_API_KEY` in your `.env` file with your actual credentials\n",
    "\n",
    "The credentials will be automatically loaded from your `.env` file in the next cell."
   ],
   "id": "db24ddaf7939ab38"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import env variables\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Configure your Qdrant connection\n",
    "# These credentials are loaded from the .env file\n",
    "# To set up:\n",
    "# 1. Copy .env.example to .env (if you haven't already)\n",
    "# 2. Sign up at https://cloud.qdrant.io/\n",
    "# 3. Create a cluster and get your credentials\n",
    "# 4. Update the QDRANT_URL and QDRANT_API_KEY in your .env file\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# Validate that credentials are set\n",
    "if not QDRANT_URL or not QDRANT_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"Qdrant credentials not found!\\n\"\n",
    "        \"Please set QDRANT_URL and QDRANT_API_KEY in your .env file.\\n\"\n",
    "        \"See .env.example for the expected format.\"\n",
    "    )\n",
    "\n",
    "# Snapshot configuration\n",
    "snapshot_file = \"../data/sample_collection.snapshot\"\n",
    "collection_name = snapshot_file.split('-')[0]  # \"hallucination_sample\"\n",
    "\n",
    "print(f\"✓ Qdrant URL: {QDRANT_URL}\")\n",
    "print(f\"✓ Collection name: {collection_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Connect to Qdrant and Upload Snapshot\n",
    "\n",
    "This cell will:\n",
    "1. Connect to your Qdrant Cloud instance using the credentials you provided\n",
    "2. Check if the collection already exists\n",
    "3. If not, upload the snapshot file to create the collection with pre-populated data\n",
    "\n",
    "The snapshot contains Earth Observation documents that will be used for hallucination detection."
   ],
   "id": "a31f6399eaa62b88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Check if snapshot file exists\n",
    "if not os.path.exists(snapshot_file):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Snapshot file not found: {snapshot_file}\\n\"\n",
    "        f\"Please ensure the snapshot file is in the current directory.\"\n",
    "    )\n",
    "\n",
    "print(f\"✓ Snapshot file found: {snapshot_file}\")\n",
    "print(f\"\\nConnecting to Qdrant Cloud...\")\n",
    "\n",
    "# Connect to your Qdrant Cloud instance\n",
    "try:\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "    # Test the connection\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"✓ Successfully connected to Qdrant Cloud!\")\n",
    "    print(f\"  Existing collections: {[col.name for col in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    raise Exception(\n",
    "        f\"Failed to connect to Qdrant Cloud. Please check your credentials.\\n\"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Check if collection already exists\n",
    "existing_collections = [col.name for col in qdrant_client.get_collections().collections]\n",
    "\n",
    "if collection_name in existing_collections:\n",
    "    print(f\"\\n✓ Collection '{collection_name}' already exists\")\n",
    "    # Get collection info\n",
    "    info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COLLECTION INFO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Name: {collection_name}\")\n",
    "    print(f\"Points count: {info.points_count}\")\n",
    "    print(f\"Vectors count: {info.vectors_count}\")\n",
    "    print(f\"Status: {info.status}\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\nCollection '{collection_name}' not found. Uploading snapshot...\")\n",
    "\n",
    "    # Extract the Qdrant URL without port for the API endpoint\n",
    "    # Handle both http and https URLs\n",
    "    base_url = QDRANT_URL.replace(':6333', '').replace(':443', '')\n",
    "\n",
    "    # Upload snapshot using HTTP API\n",
    "    with open(snapshot_file, 'rb') as f:\n",
    "        headers = {'api-key': QDRANT_API_KEY} if QDRANT_API_KEY else {}\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/collections/{collection_name}/snapshots/upload\",\n",
    "            files={'snapshot': f},\n",
    "            headers=headers\n",
    "        )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"✓ Snapshot uploaded successfully!\")\n",
    "\n",
    "        # Wait a moment for the collection to be created\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Verify collection was created\n",
    "        info = qdrant_client.get_collection(collection_name)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"COLLECTION INFO\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Name: {collection_name}\")\n",
    "        print(f\"Points count: {info.points_count}\")\n",
    "        print(f\"Vectors count: {info.vectors_count}\")\n",
    "        print(f\"Status: {info.status}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Failed to upload snapshot: {response.status_code}\\n\"\n",
    "            f\"Response: {response.text}\"\n",
    "        )\n",
    "\n",
    "print(\"\\n✓ Vector database is ready for use!\")"
   ],
   "id": "488f2dd051fb9340",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4mxxntcavmm",
   "source": "# Create keyword index for file_path field to enable filtering\nfrom qdrant_client import models\n\ntry:\n    # Check if index exists by trying to create it\n    # If it already exists, this will just update it\n    qdrant_client.create_payload_index(\n        collection_name=collection_name,\n        field_name=\"file_path\",\n        field_schema=models.PayloadSchemaType.KEYWORD\n    )\n    print(f\"✓ Created/updated keyword index for 'file_path' field\")\nexcept Exception as e:\n    print(f\"Note: Index may already exist or error occurred: {e}\")\n\nprint(f\"✓ Collection '{collection_name}' is ready with required indexes\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Example: Query the vector database to get relevant documents for a question\n",
    "def get_relevant_docs(question: str, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the vector database for a given question.\n",
    "\n",
    "    Args:\n",
    "        question: The question to search for\n",
    "        k: Number of relevant documents to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Concatenated string of relevant document contents\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedder.encode([question])[0].tolist()\n",
    "\n",
    "    # Query the Qdrant server directly\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding,\n",
    "        limit=k,\n",
    "        score_threshold=0.3\n",
    "    )\n",
    "\n",
    "    context = ''\n",
    "    # Extract and concatenate document contents\n",
    "    for i, point in enumerate(results.points):\n",
    "        # Try different possible content field names\n",
    "        content = point.payload.get('content', '') or point.payload.get('text', '') or str(point.payload)\n",
    "        context += f'Document {i}:\\n{content}\\n\\n'\n",
    "\n",
    "    return context\n",
    "\n",
    "# Test with the sample question\n",
    "relevant_docs = get_relevant_docs(\"hyperspectral\", k=5)\n",
    "\n",
    "# Print relevant docs\n",
    "print('Context: \\n', relevant_docs)"
   ],
   "id": "85411b134a158dd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generate evaluation dataset\n",
    "\n",
    "To evaluate the RAG system we will create two different evaluation datasets:\n",
    "1. **Extractive**: pieces of text extracted from the documents retrieved from Qdrant and used as query to the retrieval system\n",
    "2. **Generative QA dataset**: following the [Chroma](https://research.trychroma.com/evaluating-chunking) approach we generated new answers using an LLM and asking him to also extract the supporting chunks"
   ],
   "id": "c08e811c8c068991"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 1: Use a local model (loads model weights locally)\n",
    "# ============================================================================\n",
    "# model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# model = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 2: Use an API-based model (requires API key)\n",
    "# ============================================================================\n",
    "# Uncomment ONE of the options below to use an API instead of a local model\n",
    "# Make sure to set your API key in the .env file\n",
    "\n",
    "# --- OpenAI API (also supports OpenAI-compatible providers) ---\n",
    "from openai import OpenAI\n",
    "#\n",
    "# # Initialize OpenAI client\n",
    "# # This works with OpenAI and any OpenAI-compatible provider (LocalAI, Ollama, vLLM, etc.)\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")  # Defaults to OpenAI if not set\n",
    ")\n",
    "\n",
    "def model(messages, max_new_tokens=1500):\n",
    "    \"\"\"Wrapper to make OpenAI API compatible with the local model interface\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4\", \"gpt-3.5-turbo\", or local model name for compatible providers\n",
    "        messages=messages,\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return [{\n",
    "        'generated_text': messages + [{\n",
    "            'role': 'assistant',\n",
    "            'content': response.choices[0].message.content\n",
    "        }]\n",
    "    }]\n",
    "\n",
    "# print(f\"✓ Model loaded successfully!\")"
   ],
   "id": "e0c769b4be0cb58b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qa_generation=\"\"\"You are an agent that generates questions from a provided research paper. Your job is to generate one specific question and provide the relevant sections from the paper as references.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Generate a question that can be answered solely by the facts in the provided paper.\n",
    "\n",
    "Extract up to 5 significant sections from the paper that answer the question. These must be *exact copies* from the text and should be whole sentences where possible.\n",
    "\n",
    "Focus on the most relevant information; avoid background or unrelated sections.\n",
    "\n",
    "Format the response in JSON with three fields:\n",
    "\n",
    "\"oath\": \"I will not use the word 'and' in the question unless it is part of a proper noun. I will also make sure the question is concise.\"\n",
    "\n",
    "\"question\": A concise question directly answerable using the references.\n",
    "\n",
    "\"references\": A list of the extracted sections from the paper.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Make the question specific; do not ask about multiple topics.\n",
    "\n",
    "DO NOT USE THE WORD 'and' IN THE QUESTION UNLESS IT IS PART OF A PROPER NOUN.\n",
    "\n",
    "Do not repeat a question that has already been used.\n",
    "\n",
    "When the paper is long, scan all sections but only pick the most relevant ones to answer the question.\n",
    "\n",
    "Example:\n",
    "\n",
    "Paper Text:\n",
    "\"Section 1: Introduction: Climate change has accelerated glacier melt in the Himalayas, affecting water resources downstream.\n",
    "\n",
    "Section 2: Methodology: Remote sensing data from 2000–2020 were analyzed to quantify changes in glacier area.\n",
    "\n",
    "Section 3: Results: Glacier area decreased by 12% over 20 years, with the highest retreat in the eastern Himalayas. Streamflow measurements confirmed increased seasonal variability.\n",
    "\n",
    "Section 4: Discussion: The retreat impacts hydropower generation and agriculture. Communities relying on glacier-fed rivers experience water stress during summer months.\n",
    "\n",
    "Section 5: Conclusion: Urgent adaptation strategies are needed to mitigate the socioeconomic impact of glacier retreat.\"\n",
    "\n",
    "\n",
    "Example Output:\n",
    "{\n",
    "  \"oath\": \"I will not use the word 'and' in the question unless it is part of a proper noun. I will also make sure the question is concise.\",\n",
    "  \"question\": \"How has glacier retreat affected downstream water resources in the Himalayas?\",\n",
    "  \"references\": [\n",
    "    \"Section 3: Results: Glacier area decreased by 12% over 20 years, with the highest retreat in the eastern Himalayas. Streamflow measurements confirmed increased seasonal variability.\",\n",
    "    \"Section 4: Discussion: The retreat impacts hydropower generation and agriculture. Communities relying on glacier-fed rivers experience water stress during summer months.\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "Please provide your answer in the following JSON format:\n",
    "{format_instructions}\n",
    "\"\"\""
   ],
   "id": "152eea58303ee30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class GeneratedQA(BaseModel):\n",
    "    oath: str = Field(..., description=\"The oath taken by the agent regarding the use of the word 'and'\")\n",
    "    question: str = Field(..., description=\"A concise question directly answerable using the references\")\n",
    "    references: List[str] = Field(..., description=\"A list of extracted sections from the paper that answer the question\")\n",
    "\n",
    "def get_structured_output(model, prompt: str, schema_class: BaseModel):\n",
    "    \"\"\"\n",
    "    Helper function to get structured output from a language model.\n",
    "\n",
    "    Args:\n",
    "        model: The language model pipeline\n",
    "        prompt: The formatted prompt string (can contain {format_instructions} placeholder)\n",
    "        schema_class: The Pydantic schema class for output parsing\n",
    "\n",
    "    Returns:\n",
    "        Parsed output according to the schema, or None if parsing fails\n",
    "    \"\"\"\n",
    "    parser = PydanticOutputParser(pydantic_object=schema_class)\n",
    "\n",
    "    # Use string replacement instead of .format() to avoid issues with curly braces in the content\n",
    "    if '{format_instructions}' in prompt:\n",
    "        full_prompt = prompt.replace('{format_instructions}', parser.get_format_instructions())\n",
    "    else:\n",
    "        full_prompt = prompt\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        response = model(messages, max_new_tokens=1500)[0]['generated_text'][-1]['content']\n",
    "        output = parser.parse(response)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        return None"
   ],
   "id": "3e20392a7426e952",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's generate the dataset by sampling documents from our knowledge base. In `documents.jsonl` each line corresponds to a document with its content and metadata including the file path.",
   "id": "1e040c6539d330da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sample documents from documents.jsonl\n",
    "import json\n",
    "import random\n",
    "\n",
    "def sample_documents_from_jsonl(jsonl_path: str, n_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Sample documents from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: Path to the JSONL file\n",
    "        n_samples: Number of documents to sample\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (content, file_path)\n",
    "    \"\"\"\n",
    "    # Read all documents from JSONL\n",
    "    all_docs = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            content = doc.get('content', '')\n",
    "            file_path = doc.get('metadata', {}).get('file_path', '')\n",
    "            if content and file_path:\n",
    "                all_docs.append((content, file_path))\n",
    "    \n",
    "    print(f\"✓ Loaded {len(all_docs)} documents from {jsonl_path}\")\n",
    "    \n",
    "    # Randomly sample documents\n",
    "    sampled = random.sample(all_docs, min(n_samples, len(all_docs)))\n",
    "    \n",
    "    print(f\"✓ Sampled {len(sampled)} documents\")\n",
    "    \n",
    "    return sampled\n",
    "\n",
    "# Sample from documents.jsonl\n",
    "jsonl_path = \"../data/documents.jsonl\"\n",
    "sampled_docs_with_paths = sample_documents_from_jsonl(jsonl_path, n_samples=5)"
   ],
   "id": "45aa367b2a5ba252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Display sampled documents count\nprint(f\"✓ Ready to generate Q&A for {len(sampled_docs_with_paths)} documents\")",
   "id": "c730873cfc494cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we have the sampled documents we want also to retrieve all chunks corresponding to each document from Qdrant. This will allow us to map the references generated by the LLM back to the chunk numbers in the vector database and compute passage-level metrics.",
   "id": "7a30db63c7527185"
  },
  {
   "cell_type": "code",
   "id": "txnyo90xsni",
   "source": "def get_document_chunks_from_qdrant(client: QdrantClient, collection_name: str, file_path: str):\n    \"\"\"\n    Retrieve all chunks for a specific document from Qdrant.\n    \n    Args:\n        client: Qdrant client instance\n        collection_name: Name of the collection\n        file_path: File path to filter by\n    \n    Returns:\n        List of tuples (chunk_number, content)\n    \"\"\"\n    from qdrant_client import models\n    \n    # Scroll through Qdrant to find all chunks with matching file_path\n    chunks = []\n    offset = None\n    \n    while True:\n        records, offset = client.scroll(\n            collection_name=collection_name,\n            scroll_filter=models.Filter(\n                must=[\n                    models.FieldCondition(\n                        key=\"file_path\",\n                        match=models.MatchValue(value=file_path)\n                    )\n                ]\n            ),\n            limit=100,\n            offset=offset,\n            with_payload=True,\n            with_vectors=False\n        )\n        \n        for record in records:\n            content = record.payload.get('content', '') or record.payload.get('text', '')\n            chunks.append(content)\n        \n        if offset is None:\n            break\n    \n    # Number chunks sequentially (0-indexed)\n    numbered_chunks = [(i, chunk) for i, chunk in enumerate(chunks)]\n    return numbered_chunks\n\ndef find_reference_chunk_numbers(references: list, numbered_chunks: list, threshold: float = 0.8):\n    \"\"\"\n    Find which chunk number each reference appears in.\n    \n    Args:\n        references: List of reference texts\n        numbered_chunks: List of (chunk_number, content) tuples\n        threshold: Fuzzy matching threshold\n    \n    Returns:\n        List of tuples (reference, [chunk_numbers])\n    \"\"\"\n    reference_chunk_mapping = []\n    \n    for ref in references:\n        matching_chunks = []\n        for chunk_num, chunk_content in numbered_chunks:\n            if is_reference_present_fuzzy(ref, chunk_content, threshold):\n                matching_chunks.append(chunk_num)\n        reference_chunk_mapping.append((ref, matching_chunks))\n    \n    return reference_chunk_mapping\n\nprint(\"✓ Helper functions for chunk mapping loaded\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Q&A Generation with Reference Validation\n\nNow we generate Q&A pairs by:\n1. Providing the entire document content to the LLM\n2. Asking it to generate a question with supporting references (exact text extracts)\n3. **Validating each reference**: We check that each generated reference actually exists in at least one Qdrant chunk\n4. **Filtering invalid references**: References not found in any chunk are removed (they may have been filtered during ingestion)\n5. **Quality control**: Q&A pairs with no valid references are skipped\n\nThis validation step is crucial because:\n- Documents might be processed differently during chunking (e.g., cleaning, filtering)\n- Some text might be removed during the ingestion pipeline\n- We need ground truth references that actually exist in the retrieval system\n\nEach validated reference is mapped to its corresponding chunk numbers in Qdrant for later evaluation.",
   "id": "dacce2af39ef7c3f"
  },
  {
   "cell_type": "code",
   "id": "m0cfx4fm7kj",
   "source": "# Generate Q&A pairs for all sampled documents\nimport json\nfrom tqdm.auto import tqdm\n\nqa_dataset = []\ntotal_refs_generated = 0\ntotal_refs_valid = 0\ntotal_refs_removed = 0\nqa_pairs_skipped = 0\n\nprint(f\"Generating Q&A pairs for {len(sampled_docs_with_paths)} documents...\")\nprint(\"This may take several minutes depending on your model and hardware.\")\nprint(\"Validating that all references exist in Qdrant chunks...\\n\")\n\nfor idx, (doc_content, file_path) in enumerate(tqdm(sampled_docs_with_paths, desc=\"Generating Q&A\")):\n    input_text = f\"Context:\\n{doc_content}\\n\\nInstructions:\\n{qa_generation}\"\n    \n    # Generate structured output\n    try:\n        qa_output = get_structured_output(model, input_text, GeneratedQA)\n        \n        if qa_output:\n            # Get all chunks for this document from Qdrant\n            print(f\"\\nFetching chunks for document {idx}: {file_path}\")\n            numbered_chunks = get_document_chunks_from_qdrant(qdrant_client, collection_name, file_path)\n            print(f\"Found {len(numbered_chunks)} chunks\")\n            \n            if len(numbered_chunks) == 0:\n                print(f\"⚠ No chunks found in Qdrant for this document - skipping\")\n                qa_pairs_skipped += 1\n                continue\n            \n            # VALIDATE REFERENCES: Check if each reference exists in at least one chunk\n            validated_references = []\n            refs_generated = len(qa_output.references)\n            total_refs_generated += refs_generated\n            \n            for ref_text in qa_output.references:\n                # Check if this reference appears in at least one chunk\n                found_in_chunks = []\n                for chunk_num, chunk_content in numbered_chunks:\n                    if is_reference_present_fuzzy(ref_text, chunk_content, threshold=0.8):\n                        found_in_chunks.append(chunk_num)\n                \n                if found_in_chunks:\n                    # Reference is valid - it exists in at least one chunk\n                    validated_references.append({\n                        \"text\": ref_text,\n                        \"chunk_numbers\": found_in_chunks\n                    })\n                    total_refs_valid += 1\n                else:\n                    # Reference not found in any chunk - remove it\n                    print(f\"  ⚠ Reference not found in any chunk (removed): '{ref_text[:60]}...'\")\n                    total_refs_removed += 1\n            \n            # Only keep Q&A pair if at least one valid reference remains\n            if len(validated_references) > 0:\n                qa_entry = {\n                    \"question\": qa_output.question,\n                    \"file_path\": file_path,\n                    \"references\": validated_references,\n                    \"source_document\": doc_content[:500] + \"...\" if len(doc_content) > 500 else doc_content,\n                    \"refs_generated\": refs_generated,\n                    \"refs_valid\": len(validated_references),\n                    \"refs_removed\": refs_generated - len(validated_references)\n                }\n                qa_dataset.append(qa_entry)\n                \n                print(f\"  ✓ Valid references: {len(validated_references)}/{refs_generated}\")\n                \n                # Print progress every 10 documents\n                if (idx + 1) % 10 == 0:\n                    print(f\"\\n✓ Generated {len(qa_dataset)} Q&A pairs so far...\")\n            else:\n                print(f\"  ✗ No valid references found - skipping Q&A pair\")\n                qa_pairs_skipped += 1\n        else:\n            print(f\"\\n⚠ Failed to generate Q&A for document {idx}\")\n            qa_pairs_skipped += 1\n            \n    except Exception as e:\n        print(f\"\\n⚠ Error processing document {idx}: {e}\")\n        qa_pairs_skipped += 1\n        continue\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Q&A GENERATION COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"Total Q&A pairs generated: {len(qa_dataset)}\")\nprint(f\"Q&A pairs skipped: {qa_pairs_skipped}\")\nprint(f\"Success rate: {len(qa_dataset)/(len(sampled_docs_with_paths))*100:.1f}%\")\nprint(f\"\\nReference Statistics:\")\nprint(f\"  Total references generated: {total_refs_generated}\")\nprint(f\"  Valid references (found in chunks): {total_refs_valid}\")\nprint(f\"  Removed references (not in chunks): {total_refs_removed}\")\nprint(f\"  Validity rate: {(total_refs_valid/total_refs_generated*100) if total_refs_generated > 0 else 0:.1f}%\")\nprint(f\"{'='*60}\")\n\n# Save the dataset to a JSON file\noutput_file = \"qa_evaluation_dataset.json\"\nwith open(output_file, 'w') as f:\n    json.dump(qa_dataset, f, indent=2)\n\nprint(f\"\\n✓ Dataset saved to: {output_file}\")\n\n# Display a sample Q&A pair\nif qa_dataset:\n    print(f\"\\nSample Q&A pair:\")\n    print(f\"{'='*60}\")\n    sample = qa_dataset[0]\n    print(f\"Question: {sample['question']}\")\n    print(f\"File Path: {sample['file_path']}\")\n    print(f\"References: {sample['refs_valid']}/{sample['refs_generated']} valid\")\n    print(f\"\\nFirst Reference:\")\n    if sample['references']:\n        ref_info = sample['references'][0]\n        ref_text = ref_info['text']\n        chunk_nums = ref_info['chunk_numbers']\n        print(f\"  Text: {ref_text[:100]}...\")\n        print(f\"  Chunks: {chunk_nums}\")\n    print(f\"{'='*60}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Display Q&A dataset structure and validation statistics\nif qa_dataset:\n    sample = qa_dataset[0]\n    print(f\"Q&A Dataset Structure:\")\n    print(f\"{'='*60}\")\n    print(f\"Question: {sample['question']}\")\n    print(f\"File Path: {sample['file_path']}\")\n    print(f\"Number of References: {sample['refs_valid']} (out of {sample['refs_generated']} generated)\")\n    \n    if sample['refs_removed'] > 0:\n        print(f\"⚠ Removed {sample['refs_removed']} reference(s) not found in Qdrant chunks\")\n    \n    print(f\"\\nFirst Reference:\")\n    if sample['references']:\n        ref = sample['references'][0]\n        print(f\"  Text: {ref['text'][:150]}...\")\n        print(f\"  Chunk Numbers: {ref['chunk_numbers']}\")\n        print(f\"  ✓ Validated: Present in {len(ref['chunk_numbers'])} chunk(s)\")\n    print(f\"{'='*60}\")\n    \n    # Show overall validation statistics\n    print(f\"\\n\\nOverall Dataset Statistics:\")\n    print(f\"{'='*60}\")\n    total_generated = sum(qa['refs_generated'] for qa in qa_dataset)\n    total_valid = sum(qa['refs_valid'] for qa in qa_dataset)\n    total_removed = sum(qa['refs_removed'] for qa in qa_dataset)\n    \n    print(f\"Total Q&A pairs: {len(qa_dataset)}\")\n    print(f\"References generated: {total_generated}\")\n    print(f\"References validated: {total_valid} ({total_valid/total_generated*100:.1f}%)\")\n    print(f\"References removed: {total_removed} ({total_removed/total_generated*100:.1f}%)\")\n    print(f\"{'='*60}\")",
   "id": "68a8a7af348318fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation Metrics\n\nWe evaluate the RAG system using three distinct levels of metrics:\n\n### 1. Token-Level Metrics\nMeasures overlap between reference tokens and retrieved document tokens:\n- **IoU (Intersection over Union)**: Overlap of token sets\n- **Precision**: Fraction of retrieved tokens that are relevant\n- **Recall**: Fraction of relevant tokens that are retrieved\n- **F1 Score**: Harmonic mean of precision and recall\n\n### 2. Passage-Level Metrics\nTreats each reference as a discrete unit (passage) and measures retrieval effectiveness:\n- **Coverage**: Fraction of reference passages found in retrieved chunks\n- **Accuracy**: Binary metric (1 if all references found, 0 otherwise)\n- **Precision**: Fraction of retrieved chunks containing at least one reference\n- **Recall**: Same as coverage (fraction of references found)\n- **F1 Score**: Harmonic mean of precision and recall\n\n### 3. Document-Level Metrics\nMeasures whether the source document appears in the retrieval results:\n- **Coverage**: Binary metric (1 if source document retrieved, 0 otherwise)\n- **Accuracy**: Same as coverage\n- **Precision**: Fraction of retrieved chunks from the source document\n- **Recall**: Binary metric (same as coverage)\n- **Chunks Retrieved**: Count of source document chunks in top-K",
   "id": "8bf9750d6e3cd0bc"
  },
  {
   "cell_type": "code",
   "id": "r8k5eybdg1b",
   "source": "import pandas as pd\nfrom collections import Counter\nimport string\nfrom typing import List, Dict\n\ndef normalize_text(text: str) -> str:\n    \"\"\"Normalize text by lowercasing, removing punctuation, and normalizing whitespace.\"\"\"\n    text = text.lower()\n    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    text = \" \".join(text.split())\n    return text\n\ndef is_reference_present_fuzzy(reference: str, document: str, threshold: float = 0.8) -> bool:\n    \"\"\"\n    Returns True if enough of the reference tokens appear in the document.\n    \n    Args:\n        reference: Reference text to find\n        document: Document to search in\n        threshold: Fraction of tokens that must match (default: 0.8)\n    \n    Returns:\n        True if the reference is found with sufficient token overlap\n    \"\"\"\n    ref_tokens = normalize_text(reference).split()\n    doc_tokens = normalize_text(document).split()\n    if not ref_tokens:\n        return False\n    matched_tokens = sum(1 for t in ref_tokens if t in doc_tokens)\n    fraction_matched = matched_tokens / len(ref_tokens)\n    return fraction_matched >= threshold\n\n# ============================================================================\n# TOKEN-LEVEL METRICS\n# ============================================================================\n\ndef compute_token_metrics(references: List[str], retrieved_texts: List[str], threshold: float = 0.8) -> Dict[str, float]:\n    \"\"\"\n    Compute token-level IoU, precision, recall, and F1 score.\n    \n    Args:\n        references: List of reference texts that should be found\n        retrieved_texts: List of retrieved document chunks\n        threshold: Threshold for fuzzy matching (default: 0.8)\n    \n    Returns:\n        Dictionary with 'iou', 'precision', 'recall', 'f1' scores\n    \"\"\"\n    all_ref_tokens = []\n    all_doc_tokens = []\n\n    # Track which references are found\n    for ref in references:\n        found = any(is_reference_present_fuzzy(ref, doc, threshold) for doc in retrieved_texts)\n        ref_tokens = normalize_text(ref).split()\n        all_ref_tokens.extend(ref_tokens)\n        if found:\n            # Add tokens from retrieved documents\n            for doc in retrieved_texts:\n                all_doc_tokens.extend(normalize_text(doc).split())\n    \n    if not all_ref_tokens:\n        return {\"iou\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n\n    ref_counter = Counter(all_ref_tokens)\n    doc_counter = Counter(all_doc_tokens)\n\n    intersection_tokens = ref_counter & doc_counter\n    intersection_count = sum(intersection_tokens.values())\n\n    ref_count = sum(ref_counter.values())\n    doc_count = sum(doc_counter.values())\n\n    union_count = ref_count + doc_count - intersection_count\n\n    iou = intersection_count / union_count if union_count > 0 else 0.0\n    precision = intersection_count / doc_count if doc_count > 0 else 0.0\n    recall = intersection_count / ref_count if ref_count > 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n\n    return {\"iou\": iou, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# ============================================================================\n# PASSAGE-LEVEL METRICS\n# ============================================================================\n\ndef compute_passage_metrics(references: List[str], retrieved_texts: List[str], threshold: float = 0.8) -> Dict[str, float]:\n    \"\"\"\n    Compute passage-level coverage, accuracy, precision, recall, and F1 score.\n    \n    Passage-level treats each reference as a unit and checks if it appears in any retrieved chunk.\n    \n    Args:\n        references: List of reference texts (passages) that should be found\n        retrieved_texts: List of retrieved document chunks\n        threshold: Threshold for fuzzy matching (default: 0.8)\n    \n    Returns:\n        Dictionary with 'coverage', 'accuracy', 'precision', 'recall', 'f1' scores\n    \"\"\"\n    if not references:\n        return {\"coverage\": 0.0, \"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n    \n    if not retrieved_texts:\n        return {\"coverage\": 0.0, \"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n    \n    # Count how many references were found\n    found_references = 0\n    for ref in references:\n        if any(is_reference_present_fuzzy(ref, doc, threshold) for doc in retrieved_texts):\n            found_references += 1\n    \n    # Count how many retrieved chunks contain at least one reference\n    relevant_retrieved = 0\n    for doc in retrieved_texts:\n        if any(is_reference_present_fuzzy(ref, doc, threshold) for ref in references):\n            relevant_retrieved += 1\n    \n    # Coverage: fraction of references found\n    coverage = found_references / len(references)\n    \n    # Accuracy: 1 if all references found, 0 otherwise\n    accuracy = 1.0 if found_references == len(references) else 0.0\n    \n    # Precision: fraction of retrieved chunks that contain at least one reference\n    precision = relevant_retrieved / len(retrieved_texts) if retrieved_texts else 0.0\n    \n    # Recall: fraction of references that were found\n    recall = found_references / len(references)\n    \n    # F1: harmonic mean of precision and recall\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    return {\n        \"coverage\": coverage,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\n# ============================================================================\n# DOCUMENT-LEVEL METRICS\n# ============================================================================\n\ndef compute_document_metrics(source_file_path: str, retrieved_file_paths: List[str]) -> Dict[str, float]:\n    \"\"\"\n    Compute document-level coverage, accuracy, precision, and recall.\n    \n    Document-level checks if chunks from the source document appear in the retrieved results.\n    \n    Args:\n        source_file_path: File path of the source document\n        retrieved_file_paths: List of file paths of retrieved chunks\n    \n    Returns:\n        Dictionary with 'coverage', 'accuracy', 'precision', 'recall' scores\n    \"\"\"\n    if not retrieved_file_paths:\n        return {\"coverage\": 0.0, \"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n    \n    # Count how many retrieved chunks are from the source document\n    source_chunks_retrieved = sum(1 for fp in retrieved_file_paths if fp == source_file_path)\n    \n    # Coverage/Accuracy: binary - did we retrieve at least one chunk from source document?\n    coverage = 1.0 if source_chunks_retrieved > 0 else 0.0\n    accuracy = coverage  # Same as coverage for document-level\n    \n    # Precision: fraction of retrieved chunks that are from the source document\n    precision = source_chunks_retrieved / len(retrieved_file_paths)\n    \n    # Recall: For this metric, we define it as binary (did we find the source doc?)\n    # Could also be computed as fraction of source chunks retrieved, but that requires knowing total chunks\n    recall = coverage\n    \n    return {\n        \"coverage\": coverage,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"source_chunks_count\": source_chunks_retrieved\n    }\n\nprint(\"✓ Token-level, Passage-level, and Document-level metric functions loaded\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the RAG System at different K values\n",
    "\n",
    "Let's compute the evaluation metrics for our Q&A dataset by retrieving documents from Qdrant and calculating the defined metrics at multiple K values."
   ],
   "id": "bf3a7759a53a99bc"
  },
  {
   "cell_type": "code",
   "id": "8maftfz6dgj",
   "source": "# Evaluate the Q&A dataset by retrieving documents and computing metrics\nimport time\n\n# Configuration\nK_MAX = 15  # Maximum number of documents to retrieve\nK_VALUES = [3, 5, 10, 15]  # Compute metrics at these K values\nTHRESHOLD = 0.8  # Threshold for fuzzy matching\n\n# Store results for each K value\nresults_by_k = {k: [] for k in K_VALUES}\n\nprint(f\"Evaluating {len(qa_dataset)} Q&A pairs...\")\nprint(f\"Retrieving top-{K_MAX} documents for each question\")\nprint(f\"Computing metrics at K = {K_VALUES}\")\nprint(f\"Computing Token-level, Passage-level, and Document-level metrics\\n\")\n\nfor idx, qa_pair in enumerate(tqdm(qa_dataset, desc=\"Evaluating Q&A pairs\")):\n    question = qa_pair[\"question\"]\n    file_path = qa_pair[\"file_path\"]\n    references_info = qa_pair[\"references\"]  # List of dicts with 'text' and 'chunk_numbers'\n    \n    # Extract reference texts for evaluation\n    reference_texts = [ref_info[\"text\"] for ref_info in references_info]\n    \n    # Generate query embedding\n    query_embedding = embedder.encode([question])[0].tolist()\n    \n    # Measure retrieval time\n    start_time = time.time()\n    \n    # Retrieve documents from Qdrant (retrieve K_MAX)\n    search_results = qdrant_client.query_points(\n        collection_name=collection_name,\n        query=query_embedding,\n        limit=K_MAX,\n        score_threshold=0.3\n    )\n    \n    retrieval_time = time.time() - start_time\n    \n    # Extract ALL retrieved document contents and their file paths\n    all_retrieved_texts = []\n    all_retrieved_scores = []\n    all_retrieved_file_paths = []\n    for point in search_results.points:\n        content = point.payload.get('content', '') or point.payload.get('text', '') or str(point.payload)\n        all_retrieved_texts.append(content)\n        all_retrieved_scores.append(point.score)\n        all_retrieved_file_paths.append(point.payload.get('file_path', ''))\n    \n    # Compute metrics at different K values\n    for k in K_VALUES:\n        # Take only top-K results\n        retrieved_texts = all_retrieved_texts[:k]\n        retrieved_scores = all_retrieved_scores[:k]\n        retrieved_file_paths = all_retrieved_file_paths[:k]\n        \n        # ========================================================================\n        # COMPUTE TOKEN-LEVEL METRICS\n        # ========================================================================\n        token_metrics = compute_token_metrics(reference_texts, retrieved_texts, threshold=THRESHOLD)\n        \n        # ========================================================================\n        # COMPUTE PASSAGE-LEVEL METRICS\n        # ========================================================================\n        passage_metrics = compute_passage_metrics(reference_texts, retrieved_texts, threshold=THRESHOLD)\n        \n        # ========================================================================\n        # COMPUTE DOCUMENT-LEVEL METRICS\n        # ========================================================================\n        document_metrics = compute_document_metrics(file_path, retrieved_file_paths)\n        \n        # Store results with all three metric levels for this K value\n        results_by_k[k].append({\n            \"question\": question,\n            \"file_path\": file_path,\n            \"k\": k,\n            \"num_references\": len(reference_texts),\n            \"num_retrieved\": len(retrieved_texts),\n            \"retrieval_time\": retrieval_time,\n            \"avg_score\": sum(retrieved_scores) / len(retrieved_scores) if retrieved_scores else 0,\n            \n            # Token-level metrics\n            \"token_iou\": token_metrics[\"iou\"],\n            \"token_precision\": token_metrics[\"precision\"],\n            \"token_recall\": token_metrics[\"recall\"],\n            \"token_f1\": token_metrics[\"f1\"],\n            \n            # Passage-level metrics\n            \"passage_coverage\": passage_metrics[\"coverage\"],\n            \"passage_accuracy\": passage_metrics[\"accuracy\"],\n            \"passage_precision\": passage_metrics[\"precision\"],\n            \"passage_recall\": passage_metrics[\"recall\"],\n            \"passage_f1\": passage_metrics[\"f1\"],\n            \n            # Document-level metrics\n            \"doc_coverage\": document_metrics[\"coverage\"],\n            \"doc_accuracy\": document_metrics[\"accuracy\"],\n            \"doc_precision\": document_metrics[\"precision\"],\n            \"doc_recall\": document_metrics[\"recall\"],\n            \"doc_chunks_retrieved\": document_metrics[\"source_chunks_count\"]\n        })\n\n# Convert to DataFrames\ndfs_by_k = {k: pd.DataFrame(results) for k, results in results_by_k.items()}\n\nprint(f\"\\n✓ Evaluation complete: {len(qa_dataset)} Q&A pairs evaluated at K = {K_VALUES}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1t98jxlpiqi",
   "source": "## Evaluation at Multiple K Values\n\nWe retrieve K=15 documents but compute metrics at different cutoff points (K=3, 5, 10, 15) to understand:\n- How performance changes with the number of retrieved documents\n- The optimal K value for this RAG system\n- Trade-offs between retrieval quality and computational cost\n\nFor each K value, we compute:\n- **Token-level metrics**: Fine-grained text overlap\n- **Passage-level metrics**: Reference passage retrieval effectiveness\n- **Document-level metrics**: Source document retrieval success",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "toytlkzflu",
   "source": [
    "# Display average metrics for each K value and save results\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"AVERAGE EVALUATION METRICS AT DIFFERENT K VALUES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create a summary table\n",
    "summary_data = []\n",
    "\n",
    "for k in K_VALUES:\n",
    "    df = dfs_by_k[k]\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"K = {k}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Token-level metrics\n",
    "    print(f\"\\n  [TOKEN-LEVEL]\")\n",
    "    print(f\"    IoU: {df['token_iou'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Precision: {df['token_precision'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Recall: {df['token_recall'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"F1: {df['token_f1'].mean():.4f}\")\n",
    "    \n",
    "    # Passage-level metrics\n",
    "    print(f\"\\n  [PASSAGE-LEVEL]\")\n",
    "    print(f\"    Coverage: {df['passage_coverage'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Accuracy: {df['passage_accuracy'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Precision: {df['passage_precision'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Recall: {df['passage_recall'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"F1: {df['passage_f1'].mean():.4f}\")\n",
    "    \n",
    "    # Document-level metrics\n",
    "    print(f\"\\n  [DOCUMENT-LEVEL]\")\n",
    "    print(f\"    Coverage: {df['doc_coverage'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Accuracy: {df['doc_accuracy'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Precision: {df['doc_precision'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Recall: {df['doc_recall'].mean():.4f}  |  \", end=\"\")\n",
    "    print(f\"Avg Chunks: {df['doc_chunks_retrieved'].mean():.2f}\")\n",
    "    \n",
    "    # Store for summary table\n",
    "    summary_data.append({\n",
    "        'K': k,\n",
    "        'Token_F1': df['token_f1'].mean(),\n",
    "        'Passage_Coverage': df['passage_coverage'].mean(),\n",
    "        'Passage_F1': df['passage_f1'].mean(),\n",
    "        'Doc_Coverage': df['doc_coverage'].mean(),\n",
    "        'Doc_Precision': df['doc_precision'].mean(),\n",
    "        'Retrieval_Time': df['retrieval_time'].mean()\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Create summary comparison table\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(f\"\\nSUMMARY COMPARISON TABLE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(df_summary.to_string(index=False))\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "# Save summary table\n",
    "summary_file = \"evaluation_summary.csv\"\n",
    "df_summary.to_csv(summary_file, index=False)\n",
    "print(f\"✓ Summary saved to: {summary_file}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_summary",
   "id": "b434dce42961ca33",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
