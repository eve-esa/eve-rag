{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b9c8da",
   "metadata": {},
   "source": [
    "## What is Text Chunking?\n",
    "\n",
    "**Text chunking** is the process of breaking down large documents into smaller, manageable pieces called \"chunks\". \n",
    "\n",
    "In RAG (Retrieval-Augmented Generation) systems, chunking solves several critical problems:\n",
    "\n",
    "### 1. **Context Window Limitations**\n",
    "- Language models (like GPT) can only process a limited amount of text at once\n",
    "- Example: If a model can handle 4,000 tokens but your document is 10,000 tokens, you need to split it\n",
    "\n",
    "### 2. **Better Search & Retrieval**\n",
    "- Smaller chunks allow for more precise searching\n",
    "- Instead of finding a whole document, users can find the specific paragraph they need\n",
    "\n",
    "### 3. **Improved Relevance**\n",
    "- When a user asks a question, the system can retrieve the most relevant chunk(s) instead of the entire document\n",
    "- This leads to more focused and accurate answers\n",
    "\n",
    "\n",
    "## Key Chunking Concepts\n",
    "\n",
    "**Chunk Size**: How many words/tokens per chunk\n",
    "- Too small → Lose context\n",
    "- Too large → Poor search precision\n",
    "\n",
    "**Chunk Overlap**: How much chunks should overlap\n",
    "- Prevents cutting off important information at chunk boundaries\n",
    "- Helps maintain context between adjacent chunks\n",
    "\n",
    "**Preserving Structure**: Keeping paragraphs, headings, and formatting intact\n",
    "- Maintains readability and context\n",
    "- Helps with better understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f90d96a",
   "metadata": {},
   "source": [
    "# Recursive Chunking\n",
    "\n",
    "## What is Recursive Chunking?\n",
    "\n",
    "Instead of randomly cutting text, recursive chunking follows a **hierarchical approach**:\n",
    "\n",
    "1. **First**: Try to split by paragraphs (keeps ideas together)\n",
    "2. **Then**: If paragraphs are still too big, split by sentences\n",
    "3. **Finally**: If sentences are too big, split by words\n",
    "\n",
    "This ensures we **preserve meaning and structure** as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ea2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain_text_splitters.base import TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e78c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into paragraphs while preserving formatting.\n",
    "    - Takes a big block of text\n",
    "    - Breaks it into separate paragraphs\n",
    "    - Keeps the original formatting (spaces, line breaks)\n",
    "    - Removes empty paragraphs\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    return [p for p in paragraphs if p.strip()]\n",
    "\n",
    "\n",
    "def _split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences while preserving original formatting.\n",
    "    \n",
    "    - Takes a paragraph or block of text\n",
    "    - Breaks it into individual sentences\n",
    "    - Keeps the original spacing and punctuation\n",
    "    - Preserves how the text was originally formatted\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    pattern = r'(?<=[.!?])(\\s+)'\n",
    "    parts = re.split(pattern, text)\n",
    "    \n",
    "    sentences = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        sentence = parts[i] \n",
    "        \n",
    "        if i + 1 < len(parts):\n",
    "            sentence += parts[i + 1]  \n",
    "            \n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return [s for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96090f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      "Original text:\n",
      "This is the first paragraph.\n",
      "\n",
      "This is the second paragraph with some text.\n",
      "\n",
      "\n",
      "This is the third paragraph after extra empty lines.\n",
      "\n",
      "Split into paragraphs:\n",
      "Paragraph 1: This is the first paragraph.\n",
      "Paragraph 2: This is the second paragraph with some text.\n",
      "Paragraph 3: This is the third paragraph after extra empty lines.\n",
      "\n",
      "Original sentences:\n",
      "Hello world! How are you today? I'm doing great.  Let's learn about chunking!\n",
      "\n",
      "Split into sentences:\n",
      "Sentence 1: Hello world! \n",
      "Sentence 2: How are you today? \n",
      "Sentence 3: I'm doing great.  \n",
      "Sentence 4: Let's learn about chunking!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "\n",
    "# Test paragraph splitting\n",
    "test_text = \"\"\"This is the first paragraph.\n",
    "\n",
    "This is the second paragraph with some text.\n",
    "\n",
    "\n",
    "This is the third paragraph after extra empty lines.\"\"\"\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(test_text)\n",
    "\n",
    "print(\"\\nSplit into paragraphs:\")\n",
    "paragraphs = _split_paragraphs(test_text)\n",
    "for i, para in enumerate(paragraphs, 1):\n",
    "    print(f\"Paragraph {i}: {para}\")\n",
    "\n",
    "test_sentence = \"Hello world! How are you today? I'm doing great.  Let's learn about chunking!\"\n",
    "\n",
    "print(f\"\\nOriginal sentences:\")\n",
    "print(test_sentence)\n",
    "\n",
    "print(f\"\\nSplit into sentences:\")\n",
    "sentences = _split_sentences(test_sentence)\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd143a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveMarkdownSplitter(TextSplitter):\n",
    "    \"\"\"\n",
    "    Split text into smaller chunks while preserving meaning and structure\n",
    "    \n",
    "    This class implements our recursive chunking strategy.\n",
    "    \n",
    "    - Preserves Markdown formatting (headings, code blocks, etc.)\n",
    "    - Follows natural text boundaries (paragraphs → sentences → words)\n",
    "    - Configurable chunk size and overlap\n",
    "    - Maintains original spacing and formatting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 100, chunk_overlap: int = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size (int): Maximum number of words per chunk (default: 100)\n",
    "            chunk_overlap (int): How many words should overlap between chunks (default: 0)\n",
    "        \"\"\"\n",
    "        super().__init__(keep_separator=True)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        print(f\"   RecursiveMarkdownSplitter initialized!\")\n",
    "        print(f\"   Chunk size: {chunk_size} words\")\n",
    "        print(f\"   Chunk overlap: {chunk_overlap} words\")\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        return self._recursive_split(text)\n",
    "    \n",
    "    def chunk(self, text: str) -> List[str]:\n",
    "        return self._recursive_split(text)\n",
    "\n",
    "    def _count_words(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        This is a simple word counter. We split by spaces and count the results.\n",
    "        \"\"\"\n",
    "        return len(text.split())\n",
    "\n",
    "    def _recursive_split(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        This method implements our hierarchical splitting strategy:\n",
    "        \n",
    "        1. Check if text is already small enough\n",
    "        2. Try splitting by paragraphs first\n",
    "        3. If paragraphs are too big, try sentences  \n",
    "        4. If sentences are too big, split by words\n",
    "        \"\"\"\n",
    "\n",
    "        if self._count_words(text) <= self.chunk_size:\n",
    "            print(f\"text is small enough ({self._count_words(text)} words): Using as one chunk\")\n",
    "            return [text]\n",
    "\n",
    "        \n",
    "        paragraphs = _split_paragraphs(text)\n",
    "        print(f\"Found {len(paragraphs)} paragraphs\")\n",
    "        \n",
    "        if len(paragraphs) > 1:\n",
    "            chunks = []\n",
    "            current = \"\"\n",
    "            \n",
    "            for p in paragraphs:\n",
    "                test_text = current + (\"\\n\\n\" if current else \"\") + p\n",
    "                \n",
    "                if self._count_words(test_text) <= self.chunk_size or not current:\n",
    "                    current += (\"\\n\\n\" if current else \"\") + p\n",
    "                else:\n",
    "                    chunks.extend(self._recursive_split(current))\n",
    "                    current = p  # Start new chunk with this paragraph\n",
    "            \n",
    "            if current:\n",
    "                chunks.extend(self._recursive_split(current))\n",
    "                \n",
    "            print(f\"Paragraph split complete: {len(chunks)} chunks created\")\n",
    "            return chunks\n",
    "\n",
    "        sentences = _split_sentences(text)\n",
    "        print(f\" Found {len(sentences)} sentences\")\n",
    "        \n",
    "        if len(sentences) > 1:\n",
    "            chunks = []\n",
    "            current = \"\"\n",
    "            \n",
    "            for s in sentences:\n",
    "                if self._count_words(current + s) <= self.chunk_size or not current:\n",
    "                    current += s\n",
    "                else:\n",
    "                    chunks.append(current)\n",
    "                    current = s \n",
    "            \n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "                \n",
    "            print(f\"Sentence split complete: {len(chunks)} chunks created\")\n",
    "            return chunks\n",
    "\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            end = start + self.chunk_size\n",
    "            chunk_text = \" \".join(words[start:end])\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "            if self.chunk_overlap > 0:\n",
    "                start = end - self.chunk_overlap\n",
    "            else:\n",
    "                start = end\n",
    "                \n",
    "        print(f\"Word split complete: {len(chunks)} chunks created\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534cb43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Testing with chunk size: 15 words\n",
      "----------------------------------------\n",
      "   RecursiveMarkdownSplitter initialized!\n",
      "   Chunk size: 15 words\n",
      "   Chunk overlap: 0 words\n",
      "Found 6 paragraphs\n",
      "text is small enough (3 words): Using as one chunk\n",
      "Found 1 paragraphs\n",
      " Found 2 sentences\n",
      "Sentence split complete: 2 chunks created\n",
      "Found 1 paragraphs\n",
      " Found 2 sentences\n",
      "Sentence split complete: 2 chunks created\n",
      "text is small enough (10 words): Using as one chunk\n",
      "text is small enough (13 words): Using as one chunk\n",
      "Paragraph split complete: 7 chunks created\n",
      "\n",
      "Results: 7 chunks created\n",
      "\n",
      "--- Chunk 1 (3 words) ---\n",
      "Content:\n",
      "\n",
      "# Example Document\n",
      "\n",
      "--- Chunk 2 (12 words) ---\n",
      "Content:\n",
      "This is a short example demonstrating how the Recursive Markdown\n",
      "Splitter works. \n",
      "\n",
      "--- Chunk 3 (8 words) ---\n",
      "Content:\n",
      "It keeps paragraphs and sentence spacing in place.\n",
      "\n",
      "--- Chunk 4 (8 words) ---\n",
      "Content:\n",
      "Here is another paragraph to force paragraph-level splitting. \n",
      "\n",
      "--- Chunk 5 (18 words) ---\n",
      "Content:\n",
      "This paragraph is intentionally longer to show how the chunker handles content that exceeds the chunk size limit.\n",
      "\n",
      "--- Chunk 6 (10 words) ---\n",
      "Content:\n",
      "## Code Example\n",
      "\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, chunking world!\")\n",
      "```\n",
      "\n",
      "--- Chunk 7 (13 words) ---\n",
      "Content:\n",
      "This demonstrates that code blocks and formatting are preserved properly in our chunks.\n",
      "\n",
      "\n",
      "Testing with chunk size: 25 words\n",
      "----------------------------------------\n",
      "   RecursiveMarkdownSplitter initialized!\n",
      "   Chunk size: 25 words\n",
      "   Chunk overlap: 0 words\n",
      "Found 6 paragraphs\n",
      "text is small enough (23 words): Using as one chunk\n",
      "Found 1 paragraphs\n",
      " Found 2 sentences\n",
      "Sentence split complete: 2 chunks created\n",
      "text is small enough (23 words): Using as one chunk\n",
      "Paragraph split complete: 4 chunks created\n",
      "\n",
      "Results: 4 chunks created\n",
      "\n",
      "--- Chunk 1 (23 words) ---\n",
      "Content:\n",
      "\n",
      "# Example Document\n",
      "\n",
      "This is a short example demonstrating how the Recursive Markdown\n",
      "Splitter works. It keeps paragraphs and sentence spacing in place.\n",
      "\n",
      "--- Chunk 2 (8 words) ---\n",
      "Content:\n",
      "Here is another paragraph to force paragraph-level splitting. \n",
      "\n",
      "--- Chunk 3 (18 words) ---\n",
      "Content:\n",
      "This paragraph is intentionally longer to show how the chunker handles content that exceeds the chunk size limit.\n",
      "\n",
      "--- Chunk 4 (23 words) ---\n",
      "Content:\n",
      "## Code Example\n",
      "\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, chunking world!\")\n",
      "```\n",
      "\n",
      "This demonstrates that code blocks and formatting are preserved properly in our chunks.\n",
      "\n",
      "\n",
      "Testing with chunk size: 50 words\n",
      "----------------------------------------\n",
      "   RecursiveMarkdownSplitter initialized!\n",
      "   Chunk size: 50 words\n",
      "   Chunk overlap: 0 words\n",
      "Found 6 paragraphs\n",
      "text is small enough (49 words): Using as one chunk\n",
      "text is small enough (23 words): Using as one chunk\n",
      "Paragraph split complete: 2 chunks created\n",
      "\n",
      "Results: 2 chunks created\n",
      "\n",
      "--- Chunk 1 (49 words) ---\n",
      "Content:\n",
      "\n",
      "# Example Document\n",
      "\n",
      "This is a short example demonstrating how the Recursive Markdown\n",
      "Splitter works. It keeps paragraphs and sentence spacing in place.\n",
      "\n",
      "Here is another paragraph to force paragraph-level splitting. This paragraph is intentionally longer to show how the chunker handles content that exceeds the chunk size limit.\n",
      "\n",
      "--- Chunk 2 (23 words) ---\n",
      "Content:\n",
      "## Code Example\n",
      "\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, chunking world!\")\n",
      "```\n",
      "\n",
      "This demonstrates that code blocks and formatting are preserved properly in our chunks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = \"\"\"\n",
    "# Example Document\n",
    "\n",
    "This is a short example demonstrating how the Recursive Markdown\n",
    "Splitter works. It keeps paragraphs and sentence spacing in place.\n",
    "\n",
    "Here is another paragraph to force paragraph-level splitting. This paragraph is intentionally longer to show how the chunker handles content that exceeds the chunk size limit.\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "def hello_world():\n",
    "    print(\"Hello, chunking world!\")\n",
    "```\n",
    "\n",
    "This demonstrates that code blocks and formatting are preserved properly in our chunks.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# testest with different chunk sizes to see the behavior\n",
    "chunk_sizes = [15, 25, 50]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    print(f\"\\nTesting with chunk size: {size} words\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    splitter = RecursiveMarkdownSplitter(chunk_size=size)\n",
    "    \n",
    "    chunks = splitter.chunk(sample)\n",
    "    \n",
    "    print(f\"\\nResults: {len(chunks)} chunks created\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        word_count = splitter._count_words(chunk)\n",
    "        print(f\"\\n--- Chunk {i} ({word_count} words) ---\")\n",
    "        print(\"Content:\")\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d49d8c",
   "metadata": {},
   "source": [
    "# Hierarchial Chunking\n",
    "\n",
    "## What is Hierarchial Chunking?\n",
    "\n",
    "This is a two step chunking strategy where -\n",
    "\n",
    "1. **First**: Split the document into logical sections based on Markdown headers\n",
    "2. **Then**: If any section exceeds the max chunk size, apply a secondary chunking method\n",
    "       that preserves LaTeX formulas and tables\n",
    "3. **Finally**: Try to aggregate subsections into sections when possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d210f67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('##', 2), ('###', 3), ('####', 4), ('#####', 5), ('######', 6)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "levels = [1, 2, 3, 4, 5, 6]\n",
    "headers_to_split_on = [(\"#\" * level, level) for level in levels]\n",
    "headers_to_split_on # we will be splitting the markdown on all these levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfed99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "# Example Document\n",
    "\n",
    "This is a short example text.\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "def hello_world():\n",
    "    print(\"Hello, chunking world!\")\n",
    "```\n",
    "\n",
    "This demonstrates that code blocks and formatting are preserved properly in our chunks.\n",
    "\n",
    "### Comments Example\n",
    "\n",
    "also make sure to add comments in your code\n",
    "\"\"\"\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "sections = markdown_splitter.split_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67540e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Section 1 ---\n",
      "page_content='# Example Document  \n",
      "This is a short example text.' metadata={1: 'Example Document'}\n",
      "\n",
      "--- Section 2 ---\n",
      "page_content='## Code Example  \n",
      "```python\n",
      "def hello_world():\n",
      "print(\"Hello, chunking world!\")\n",
      "```  \n",
      "This demonstrates that code blocks and formatting are preserved properly in our chunks.' metadata={1: 'Example Document', 2: 'Code Example'}\n",
      "\n",
      "--- Section 3 ---\n",
      "page_content='### Comments Example  \n",
      "also make sure to add comments in your code' metadata={1: 'Example Document', 2: 'Code Example', 3: 'Comments Example'}\n"
     ]
    }
   ],
   "source": [
    "for idx, section in enumerate(sections):\n",
    "    print(f\"\\n--- Section {idx + 1} ---\")\n",
    "    print(section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd5f16",
   "metadata": {},
   "source": [
    "## Sentence Text Splitter\n",
    "\n",
    "Now that we have divided text into meaningful sections, we need to divide them into smaller fragments like sentences while preserving meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e3dbc",
   "metadata": {},
   "source": [
    "Let's write a utility to find latex environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42e36708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_latex_environments(text):\n",
    "    \"\"\"\n",
    "    Identify all LaTeX environments in the text, handling nested environments correctly.\n",
    "\n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "\n",
    "    Returns:\n",
    "        List of (start, end) tuples for all LaTeX environments\n",
    "    \"\"\"\n",
    "    environments = []\n",
    "    pos = 0\n",
    "\n",
    "    while True:\n",
    "        # Find the next \\begin\n",
    "        begin_pos = text.find(\"\\\\begin{\", pos)\n",
    "        if begin_pos == -1:\n",
    "            break\n",
    "\n",
    "        # Find the matching \\end\n",
    "        end_pos = find_matching_end(text, begin_pos)\n",
    "        if end_pos == -1:\n",
    "            # Skip this \\begin if there's no matching \\end\n",
    "            pos = begin_pos + 6  # Move past \"\\begin\"\n",
    "            continue\n",
    "\n",
    "        environments.append((begin_pos, end_pos))\n",
    "        pos = end_pos\n",
    "\n",
    "    return environments\n",
    "\n",
    "def find_matching_end(text, begin_pos):\n",
    "    \"\"\"\n",
    "    Find the matching \\\\end{...} for a \\\\begin{...} at the given position.\n",
    "    Handles nested environments correctly.\n",
    "\n",
    "    Args:\n",
    "        text: The text to search in\n",
    "        begin_pos: Position of the \\\\begin{...} command\n",
    "\n",
    "    Returns:\n",
    "        Position of the end of the matching \\\\end{...} command or -1 if not found\n",
    "    \"\"\"\n",
    "    # Extract the environment name\n",
    "    begin_match = re.search(r'\\\\begin\\{([^}]+)\\}', text[begin_pos:])\n",
    "    if not begin_match:\n",
    "        return -1\n",
    "\n",
    "    env_name = begin_match.group(1)\n",
    "    env_begin = f\"\\\\begin{{{env_name}}}\"\n",
    "    env_end = f\"\\\\end{{{env_name}}}\"\n",
    "\n",
    "    # Find the end of the current \\begin command\n",
    "    current_pos = begin_pos + len(env_begin)\n",
    "    nesting_level = 1\n",
    "\n",
    "    while nesting_level > 0 and current_pos < len(text):\n",
    "        # Look for the next \\begin or \\end of the same environment\n",
    "        begin_idx = text.find(env_begin, current_pos)\n",
    "        end_idx = text.find(env_end, current_pos)\n",
    "\n",
    "        # If no more begin/end tags, environment is not properly closed\n",
    "        if end_idx == -1:\n",
    "            return -1\n",
    "\n",
    "        # If we find an end tag first or no more begin tags\n",
    "        if begin_idx == -1 or end_idx < begin_idx:\n",
    "            nesting_level -= 1\n",
    "            current_pos = end_idx + len(env_end)\n",
    "        else:\n",
    "            nesting_level += 1\n",
    "            current_pos = begin_idx + len(env_begin)\n",
    "\n",
    "    return current_pos if nesting_level == 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f048000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{figure}\n",
      "    Some figure content.\n",
      "\n",
      "    \\begin{center}\n",
      "        This is centered text.\n",
      "    \\end{center}\n",
      "\n",
      "    More figure content.\n",
      "\\end{figure}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Here is some text.\n",
    "\n",
    "\\\\begin{figure}\n",
    "    Some figure content.\n",
    "\n",
    "    \\\\begin{center}\n",
    "        This is centered text.\n",
    "    \\\\end{center}\n",
    "\n",
    "    More figure content.\n",
    "\\\\end{figure}\n",
    "\n",
    "End of document.\n",
    "\"\"\"\n",
    "\n",
    "envs = find_latex_environments(text)\n",
    "print(text[20:166])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962ba80",
   "metadata": {},
   "source": [
    "Along with latex environments, we also need to preserve other segments like markdown tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e85677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tables(text):\n",
    "    markdown_tables = []\n",
    "    table_pattern = re.compile(r'(\\|[^\\n]+\\|\\n)((?:\\|[^\\n]+\\|\\n)+)')\n",
    "    for match in table_pattern.finditer(text):\n",
    "        markdown_tables.append((match.start(), match.end()))\n",
    "    return markdown_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdafffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(37, 103), (132, 192)]\n",
      "| Name | Age | City |\n",
      "| John | 30  | NY   |\n",
      "| Ana  | 22  | LA   |\n",
      " | Product | Price |\n",
      "| Apple   | 1.2   |\n",
      "| Orange  | 0.8   |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Here is some text before the table.\n",
    "\n",
    "| Name | Age | City |\n",
    "| John | 30  | NY   |\n",
    "| Ana  | 22  | LA   |\n",
    "\n",
    "Some text after the table.\n",
    "\n",
    "| Product | Price |\n",
    "| Apple   | 1.2   |\n",
    "| Orange  | 0.8   |\n",
    "\"\"\"\n",
    "\n",
    "tables_idx = find_tables(text)\n",
    "print(tables_idx)\n",
    "print(text[37:103], text[132: 192])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e63e1d",
   "metadata": {},
   "source": [
    "Put them together as a utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0406a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_preserved_spans(text):\n",
    "    \"\"\"\n",
    "    Identify all spans in the text that should be preserved atomically.\n",
    "\n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "\n",
    "    Returns:\n",
    "        List of (start, end) tuples for preserved spans\n",
    "    \"\"\"\n",
    "    preserved_spans = []\n",
    "\n",
    "    # Find LaTeX environments (tables, equations, etc.)\n",
    "    preserved_spans.extend(find_latex_environments(text))\n",
    "\n",
    "    # Find markdown tables\n",
    "    table_pattern = re.compile(r'(\\|[^\\n]+\\|\\n)((?:\\|[^\\n]+\\|\\n)+)')\n",
    "    for match in table_pattern.finditer(text):\n",
    "        is_inside_env = any(start <= match.start() and match.end() <= end\n",
    "                            for start, end in preserved_spans)\n",
    "        if not is_inside_env:\n",
    "            preserved_spans.append((match.start(), match.end()))\n",
    "\n",
    "    # Sort and merge overlapping spans\n",
    "    if preserved_spans:\n",
    "        preserved_spans.sort()\n",
    "        merged_spans = []\n",
    "        current_start, current_end = preserved_spans[0]\n",
    "\n",
    "        for start, end in preserved_spans[1:]:\n",
    "            if start <= current_end:  # Spans overlap\n",
    "                current_end = max(current_end, end)\n",
    "            else:  # No overlap\n",
    "                merged_spans.append((current_start, current_end))\n",
    "                current_start, current_end = start, end\n",
    "\n",
    "        merged_spans.append((current_start, current_end))\n",
    "        preserved_spans = merged_spans\n",
    "\n",
    "    return preserved_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b7251",
   "metadata": {},
   "source": [
    "Now we are ready to split text into chunks based on sentences while preserving LaTeX content and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b20917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenize_with_protection(text):\n",
    "    # Store patterns that should be protected\n",
    "    protected_patterns = []\n",
    "\n",
    "    # Find and replace LaTeX formulas with placeholders\n",
    "    def replace_protected(match):\n",
    "        protected_patterns.append(match.group(0))\n",
    "        return f\"PROTECTED_PLACEHOLDER_{len(protected_patterns) - 1}\"\n",
    "\n",
    "    # Pattern to match LaTeX formulas enclosed in \\[ \\] or $ $\n",
    "    latex_pattern = r'\\\\\\[.*?\\\\\\]|\\$.*?\\$'\n",
    "\n",
    "    # Pattern to match figure references like \"Fig. 2:\" or \"Table 1.\"\n",
    "    figure_pattern = r'(Fig\\.|Figure|Tab\\.|Table|Eq\\.|Equation)\\s+\\d+[\\.:][^\\.]*?'\n",
    "\n",
    "    # Combine patterns\n",
    "    combined_pattern = f\"({latex_pattern})|({figure_pattern})\"\n",
    "\n",
    "    # Replace protected elements with placeholders\n",
    "    protected_text = re.sub(combined_pattern, replace_protected, text, flags=re.DOTALL)\n",
    "\n",
    "    # Tokenize the protected text\n",
    "    sentences = nltk.sent_tokenize(protected_text)\n",
    "\n",
    "    # Restore protected elements\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, protected in enumerate(protected_patterns):\n",
    "            sentences[i] = sentences[i].replace(f\"PROTECTED_PLACEHOLDER_{j}\", protected)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f89ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    preserved_spans = identify_preserved_spans(text)\n",
    "\n",
    "    # Tokenize text into sentences, but skip the preserved spans\n",
    "    sentences = []\n",
    "    last_end = 0\n",
    "\n",
    "    for start, end in preserved_spans:\n",
    "        # Process text before the preserved span\n",
    "        if start > last_end:\n",
    "            before_text = text[last_end:start]\n",
    "            if before_text.strip():\n",
    "                # Split the text before the preserved span into sentences\n",
    "                before_sentences = tokenize_with_protection(before_text)\n",
    "                sentences.extend(before_sentences)\n",
    "\n",
    "        # Add the preserved span as a single \"sentence\"\n",
    "        sentences.append(text[start:end])\n",
    "        last_end = end\n",
    "\n",
    "    # Process text after the last preserved span\n",
    "    if last_end < len(text):\n",
    "        after_text = text[last_end:]\n",
    "        if after_text.strip():\n",
    "            after_sentences = tokenize_with_protection(after_text)\n",
    "            sentences.extend(after_sentences)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c69293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Here is some text before the table.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "| Name | Age | City |\n",
      "| John | 30  | NY   |\n",
      "| Ana  | 22  | LA   |\n",
      "\n",
      "\n",
      "--- Chunk 3 ---\n",
      "\n",
      "Some text after the table.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "| Product | Price |\n",
      "| Apple   | 1.2   |\n",
      "| Orange  | 0.8   |\n",
      "\n",
      "\n",
      "--- Chunk 5 ---\n",
      "\n",
      "Here is some text.\n",
      "\n",
      "--- Chunk 6 ---\n",
      "\\begin{figure}\n",
      "    Some figure content.\n",
      "\n",
      "    \\begin{center}\n",
      "        This is centered text.\n",
      "    \\end{center}\n",
      "\n",
      "    More figure content.\n",
      "\\end{figure}\n",
      "\n",
      "--- Chunk 7 ---\n",
      "\n",
      "\n",
      "End of document.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Here is some text before the table.\n",
    "\n",
    "| Name | Age | City |\n",
    "| John | 30  | NY   |\n",
    "| Ana  | 22  | LA   |\n",
    "\n",
    "Some text after the table.\n",
    "\n",
    "| Product | Price |\n",
    "| Apple   | 1.2   |\n",
    "| Orange  | 0.8   |\n",
    "\n",
    "Here is some text.\n",
    "\n",
    "\\\\begin{figure}\n",
    "    Some figure content.\n",
    "\n",
    "    \\\\begin{center}\n",
    "        This is centered text.\n",
    "    \\\\end{center}\n",
    "\n",
    "    More figure content.\n",
    "\\\\end{figure}\n",
    "\n",
    "End of document.\n",
    "\"\"\"\n",
    "sentences = split_text(text)\n",
    "for idx, sent in enumerate(sentences):\n",
    "    print(f\"\\n--- Chunk {idx + 1} ---\")\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738dfcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eve-rag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
