{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf8ab32",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this section, we implement a complete RAG pipeline for answering questions based on a given context. Using the LangChain library, we'll walk through the entire processâ€”from retrieving relevant context to generating accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc73147",
   "metadata": {},
   "source": [
    "So far, we have done:\n",
    "1. **Indexing**: Organize the raw documents into a structured format suitable for processing, such as splitting them into chunks or passages for more efficient retrieval.\n",
    "\n",
    "2. **Embedding**: Convert each text chunk into a dense vector representation using a pre-trained embedding model. These embeddings capture the semantic meaning of the content.\n",
    "\n",
    "3. **Vector Store**: Store the embeddings in a vector database (Qdrant in our case), allowing fast and scalable similarity search across the document collection.\n",
    "\n",
    "Now we will continue with **Retrieval and Generation**: in this notebook, given a user query, retrieve the most relevant document chunks from the vector store and feed them into a language model (EVE) to generate a context-aware, accurate response."
   ]
  },
  {
   "cell_type": "code",
   "id": "62191c3c",
   "metadata": {},
   "source": "from sentence_transformers import SentenceTransformer\n\n# Initialize the embedding model\nembedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b613d244",
   "metadata": {},
   "source": "query = \"What is TROPOMI?\"\nquery_vector = embedder.encode([query])[0].tolist()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46f8624c",
   "metadata": {},
   "source": [
    "from qdrant_client import QdrantClient\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# get your keys from the qdrant UI\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "\n",
    "collection_name = \"ingestion_demo\"\n",
    "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07bef381",
   "metadata": {},
   "source": [
    "results = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=1  # number of similar results you want\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9723960a",
   "metadata": {},
   "source": [
    "for result in results.points:\n",
    "    print(f\"retrieval score: {result.score}\")\n",
    "    print(f\"retrieved chunk: {result.payload['content']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58ba6a3c",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Once the relevant context is retrieved, it is passed to an LLM to generate a coherent and informed response based on both the query and the retrieved context.\n",
    "\n",
    "This approach ensures that the generated answers are grounded in the source documents, improving accuracy and reducing hallucination."
   ]
  },
  {
   "cell_type": "code",
   "id": "a434ae1f",
   "metadata": {},
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"Qwen/Qwen3-0.6B\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b489e1a2",
   "metadata": {},
   "source": [
    "from langchain.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "system_message = '''You are an expert assistant that answers questions about different topics.\n",
    "If you don't know the answer, just say \"I don't know.\" Don't try to make up an answer.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "Do not use any prior knowledge.'''\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(\n",
    "        content=f\"Context: {result.payload['content']} Question: {query}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac9f6a67",
   "metadata": {},
   "source": [
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eve-rag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}