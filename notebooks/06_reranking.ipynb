{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Re-ranking for Improved Retrieval Quality\n\nIn this notebook, we explore **re-ranking** as a technique to improve the quality of retrieved documents in RAG systems.\n\n## What is Re-ranking?\n\nRe-ranking is a two-stage retrieval approach:\n\n1. **First Stage (Fast Retrieval)**: Use a fast embedding model (like sentence transformers) to retrieve a larger set of candidate documents (e.g., top-20 or top-50)\n2. **Second Stage (Precise Re-ranking)**: Use a more sophisticated model to re-score and re-order these candidates based on relevance\n\n## Why Re-ranking?\n\n### The Problem with Single-Stage Retrieval\n\n- **Embedding models** are fast but may not capture complex semantic relationships\n- **Cosine similarity** in vector space doesn't always reflect true relevance\n- **Top-K results** may include irrelevant documents with high embedding similarity\n\n### The Solution: Re-ranking\n\n- **Better accuracy**: Re-ranking models (like cross-encoders) process query-document pairs jointly\n- **Improved relevance**: More sophisticated models can better assess relevance\n- **Better ranking**: Moves truly relevant documents to the top positions\n- **Cost-effective**: Only re-rank a small set of candidates (not the entire corpus)\n\n## Re-ranking Model\n\nWe'll use **BAAI/bge-reranker-base**, a state-of-the-art general-purpose re-ranking model:\n\n- **Type**: Cross-encoder (sequence classification model)\n- **Domain**: General-purpose (works across all domains)\n- **Approach**: Direct relevance scoring for query-document pairs\n- **Performance**: Top results on BEIR benchmark\n\n## Evaluation Approach\n\nWe'll compare:\n- **Baseline**: Retrieval without re-ranking (embedding similarity only)\n- **With Re-ranking**: Retrieval followed by re-ranking\n\nUsing metrics:\n- Token-level: IoU, Precision, Recall, F1\n- Passage-level: Coverage, Accuracy, Precision, Recall, F1\n- Document-level: Coverage, Precision\n- Ranking quality: **Mean Reciprocal Rank (MRR)**, Rank Distribution"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {},
   "source": "# Import necessary libraries\nimport sys\nimport os\nimport json\nimport time\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple\nfrom collections import Counter\nimport string\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom tqdm.auto import tqdm\n\nsys.path.append(os.path.join(os.getcwd(), '..'))\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Configure Qdrant connection\nQDRANT_URL = os.getenv(\"QDRANT_URL\")\nQDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\nif not QDRANT_URL or not QDRANT_API_KEY:\n    raise ValueError(\n        \"Qdrant credentials not found!\\n\"\n        \"Please set QDRANT_URL and QDRANT_API_KEY in your .env file.\"\n    )\n\ncollection_name = \"sample_collection.snapshot\"\n\nprint(f\"✓ Environment configured\")\nprint(f\"  Qdrant URL: {QDRANT_URL}\")\nprint(f\"  Collection: {collection_name}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "id": "connect-qdrant",
   "metadata": {},
   "source": [
    "try:\n",
    "    qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "    info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"✓ Connected to Qdrant\")\n",
    "    print(f\"  Collection: {collection_name}\")\n",
    "    print(f\"  Points: {info.points_count}\")\n",
    "    print(f\"  Status: {info.status}\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to connect to Qdrant: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "embedder-header",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "init-embedder",
   "metadata": {},
   "source": [
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"✓ Embedding model loaded: all-MiniLM-L6-v2\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "reranker-header",
   "metadata": {},
   "source": "## Re-ranking Model Implementation\n\nWe'll use **BAAI/bge-reranker-base**, a state-of-the-art general-purpose cross-encoder reranking model.\n\n### How it Works\n\nA cross-encoder:\n1. Takes a query-document pair as input\n2. Jointly processes both through transformer layers\n3. Outputs a relevance score\n4. Is more accurate than bi-encoders (separate query/doc embeddings) but slower\n5. Perfect for re-ranking a small candidate set\n\n### BGE Reranker Advantages\n\n- **State-of-the-art**: Among the best performing rerankers on BEIR benchmark\n- **General-purpose**: Works well across diverse domains and tasks\n- **Efficient**: ~280MB model size, similar to domain-specific alternatives\n- **Production-ready**: Widely used in industry applications\n- **Practical**: Can run locally without massive GPU requirements"
  },
  {
   "cell_type": "code",
   "id": "indus-reranker",
   "metadata": {},
   "source": "class Reranker:\n    \"\"\"BGE Reranker implementation - state-of-the-art general-purpose reranking.\"\"\"\n    \n    def __init__(self, model_name=\"BAAI/bge-reranker-base\", device=None, max_length=512):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        self.model.eval()\n        self.max_length = max_length\n\n    def rerank(self, query: str, docs: List[str]) -> Tuple[List[str], List[float]]:\n        \"\"\"\n        Rerank documents for a query.\n        \n        Args:\n            query: The query string\n            docs: List of documents to rerank\n        \n        Returns:\n            Tuple of (sorted_docs, scores)\n        \"\"\"\n        # Tokenize query-doc pairs\n        encodings = self.tokenizer(\n            text=[query] * len(docs),\n            text_pair=docs,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Compute relevance scores\n        with torch.no_grad():\n            logits = self.model(**encodings).logits\n            # BGE reranker outputs single logit per pair\n            scores = logits.squeeze(-1).cpu().tolist()\n\n        # Sort by score descending\n        sorted_pairs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n        sorted_docs, sorted_scores = zip(*sorted_pairs) if sorted_pairs else ([], [])\n\n        return list(sorted_docs), list(sorted_scores)\n\nprint(\"✓ Reranker class defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "init-reranker-header",
   "metadata": {},
   "source": "## Initialize Re-ranker\n\nWe'll initialize the BGE reranker model.\n\n**Note**: The first run will download the model weights (~280MB)."
  },
  {
   "cell_type": "code",
   "id": "init-reranker",
   "metadata": {},
   "source": "# Initialize the BGE reranker\nreranker = Reranker(max_length=512)\nprint(\"✓ BGE Reranker initialized (BAAI/bge-reranker-base)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "load-qa-header",
   "metadata": {},
   "source": [
    "## Load Evaluation Dataset\n",
    "\n",
    "We'll use the same Q&A dataset from the evaluation notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "load-qa",
   "metadata": {},
   "source": [
    "# Load evaluation dataset\n",
    "qa_dataset_file = \"../data/qa_evaluation_dataset.json\"\n",
    "\n",
    "if os.path.exists(qa_dataset_file):\n",
    "    with open(qa_dataset_file, 'r') as f:\n",
    "        qa_dataset = json.load(f)\n",
    "    print(f\"✓ Loaded {len(qa_dataset)} Q&A pairs from {qa_dataset_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Evaluation dataset not found at {qa_dataset_file}\\n\"\n",
    "        f\"Please ensure the Q&A dataset has been generated first.\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Using {len(qa_dataset)} Q&A pairs for evaluation\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We use comprehensive metrics at multiple levels:\n",
    "\n",
    "### Token-Level Metrics\n",
    "- **IoU**: Intersection over Union of token sets\n",
    "- **Precision**: Fraction of retrieved tokens that are relevant\n",
    "- **Recall**: Fraction of relevant tokens that are retrieved\n",
    "- **F1**: Harmonic mean of precision and recall\n",
    "\n",
    "### Passage-Level Metrics\n",
    "- **Coverage**: Fraction of reference passages found\n",
    "- **Accuracy**: Binary metric (all references found or not)\n",
    "- **Precision**: Fraction of retrieved chunks containing references\n",
    "- **Recall**: Same as coverage\n",
    "- **F1**: Harmonic mean of precision and recall\n",
    "\n",
    "### Document-Level Metrics\n",
    "- **Coverage**: Whether source document appears in results\n",
    "- **Precision**: Fraction of retrieved chunks from source document\n",
    "\n",
    "### Ranking Quality Metrics (NEW)\n",
    "- **Mean Reciprocal Rank (MRR)**: Average of 1/rank for first relevant document\n",
    "  - MRR = 1.0 means relevant doc at rank 1\n",
    "  - MRR = 0.5 means relevant doc at rank 2\n",
    "  - MRR = 0.33 means relevant doc at rank 3\n",
    "- **Rank Distribution**: Where relevant documents appear in the ranking"
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics-functions",
   "metadata": {},
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def is_reference_present_fuzzy(reference: str, document: str, threshold: float = 0.8) -> bool:\n",
    "    \"\"\"Check if reference appears in document with fuzzy matching.\"\"\"\n",
    "    ref_tokens = normalize_text(reference).split()\n",
    "    doc_tokens = normalize_text(document).split()\n",
    "    if not ref_tokens:\n",
    "        return False\n",
    "    matched_tokens = sum(1 for t in ref_tokens if t in doc_tokens)\n",
    "    fraction_matched = matched_tokens / len(ref_tokens)\n",
    "    return fraction_matched >= threshold\n",
    "\n",
    "def compute_token_metrics(references: List[str], retrieved_texts: List[str], threshold: float = 0.8) -> Dict[str, float]:\n",
    "    \"\"\"Compute token-level metrics.\"\"\"\n",
    "    all_ref_tokens = []\n",
    "    all_doc_tokens = []\n",
    "\n",
    "    for ref in references:\n",
    "        found = any(is_reference_present_fuzzy(ref, doc, threshold) for doc in retrieved_texts)\n",
    "        ref_tokens = normalize_text(ref).split()\n",
    "        all_ref_tokens.extend(ref_tokens)\n",
    "        if found:\n",
    "            for doc in retrieved_texts:\n",
    "                all_doc_tokens.extend(normalize_text(doc).split())\n",
    "    \n",
    "    if not all_ref_tokens:\n",
    "        return {\"iou\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    ref_counter = Counter(all_ref_tokens)\n",
    "    doc_counter = Counter(all_doc_tokens)\n",
    "\n",
    "    intersection_tokens = ref_counter & doc_counter\n",
    "    intersection_count = sum(intersection_tokens.values())\n",
    "\n",
    "    ref_count = sum(ref_counter.values())\n",
    "    doc_count = sum(doc_counter.values())\n",
    "    union_count = ref_count + doc_count - intersection_count\n",
    "\n",
    "    iou = intersection_count / union_count if union_count > 0 else 0.0\n",
    "    precision = intersection_count / doc_count if doc_count > 0 else 0.0\n",
    "    recall = intersection_count / ref_count if ref_count > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\"iou\": iou, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def compute_passage_metrics(references: List[str], retrieved_texts: List[str], threshold: float = 0.8) -> Dict[str, float]:\n",
    "    \"\"\"Compute passage-level metrics.\"\"\"\n",
    "    if not references or not retrieved_texts:\n",
    "        return {\"coverage\": 0.0, \"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    found_references = sum(1 for ref in references if any(is_reference_present_fuzzy(ref, doc, threshold) for doc in retrieved_texts))\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_texts if any(is_reference_present_fuzzy(ref, doc, threshold) for ref in references))\n",
    "    \n",
    "    coverage = found_references / len(references)\n",
    "    accuracy = 1.0 if found_references == len(references) else 0.0\n",
    "    precision = relevant_retrieved / len(retrieved_texts) if retrieved_texts else 0.0\n",
    "    recall = coverage\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\"coverage\": coverage, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def compute_document_metrics(source_file_path: str, retrieved_file_paths: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Compute document-level metrics.\"\"\"\n",
    "    if not retrieved_file_paths:\n",
    "        return {\"coverage\": 0.0, \"precision\": 0.0}\n",
    "    \n",
    "    source_chunks_retrieved = sum(1 for fp in retrieved_file_paths if fp == source_file_path)\n",
    "    coverage = 1.0 if source_chunks_retrieved > 0 else 0.0\n",
    "    precision = source_chunks_retrieved / len(retrieved_file_paths)\n",
    "    \n",
    "    return {\"coverage\": coverage, \"precision\": precision, \"source_chunks_count\": source_chunks_retrieved}\n",
    "\n",
    "def compute_mrr(references: List[str], retrieved_texts: List[str], threshold: float = 0.8) -> Dict[str, float]:\n",
    "    \"\"\"Compute Mean Reciprocal Rank - measures ranking quality.\"\"\"\n",
    "    for rank, doc in enumerate(retrieved_texts, start=1):\n",
    "        if any(is_reference_present_fuzzy(ref, doc, threshold) for ref in references):\n",
    "            return {\"reciprocal_rank\": 1.0 / rank, \"first_relevant_rank\": rank}\n",
    "    return {\"reciprocal_rank\": 0.0, \"first_relevant_rank\": None}\n",
    "\n",
    "print(\"✓ Metric functions defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-header",
   "metadata": {},
   "source": [
    "## Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "retrieval-function",
   "metadata": {},
   "source": [
    "def retrieve_documents(query: str, k: int = 20) -> List[Dict]:\n",
    "    \"\"\"Retrieve documents from Qdrant.\"\"\"\n",
    "    query_embedding = embedder.encode([query])[0].tolist()\n",
    "    \n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding,\n",
    "        limit=k,\n",
    "        score_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    for point in results.points:\n",
    "        documents.append({\n",
    "            'content': point.payload.get('content', '') or point.payload.get('text', ''),\n",
    "            'file_path': point.payload.get('file_path', ''),\n",
    "            'score': point.score\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "print(\"✓ Retrieval function defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "demo-header",
   "metadata": {},
   "source": [
    "## Demonstration: Re-ranking in Action\n",
    "\n",
    "Let's see how re-ranking changes the order of retrieved documents for a sample query."
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-reranking",
   "metadata": {},
   "source": "# Take first Q&A pair for demonstration\nif qa_dataset:\n    demo_qa = qa_dataset[0]\n    demo_query = demo_qa['question']\n    \n    print(f\"Query: {demo_query}\")\n    print(f\"\\n{'='*80}\")\n    \n    # Retrieve documents\n    retrieved_docs = retrieve_documents(demo_query, k=10)\n    \n    print(f\"\\nRETRIEVED DOCUMENTS (Embedding-based ranking)\")\n    print(f\"{'='*80}\")\n    for i, doc in enumerate(retrieved_docs[:5], 1):\n        print(f\"\\n[{i}] Score: {doc['score']:.4f}\")\n        print(f\"    File: {doc['file_path'].split('/')[-1] if doc['file_path'] else 'N/A'}\")\n        print(f\"    Content: {doc['content'][:150]}...\")\n    \n    # Re-rank\n    doc_contents = [d['content'] for d in retrieved_docs]\n    reranked_contents, reranked_scores = reranker.rerank(demo_query, doc_contents)\n    \n    # Map back to original documents and track original positions\n    content_to_doc = {d['content']: d for d in retrieved_docs}\n    content_to_original_pos = {d['content']: idx + 1 for idx, d in enumerate(retrieved_docs)}\n    reranked_docs = [content_to_doc[content] for content in reranked_contents]\n    \n    print(f\"\\n\\nRE-RANKED DOCUMENTS (Re-ranker scores)\")\n    print(f\"{'='*80}\")\n    for i, (doc, score) in enumerate(zip(reranked_docs[:5], reranked_scores[:5]), 1):\n        original_pos = content_to_original_pos[doc['content']]\n        position_change = original_pos - i\n        \n        if position_change > 0:\n            change_indicator = f\"↑ +{position_change}\"\n        elif position_change < 0:\n            change_indicator = f\"↓ {position_change}\"\n        else:\n            change_indicator = \"=\"\n        \n        print(f\"\\n[{i}] Position: #{original_pos} → #{i} ({change_indicator})\")\n        print(f\"    Rerank Score: {score:.4f} | Original Score: {doc['score']:.4f}\")\n        print(f\"    File: {doc['file_path'].split('/')[-1] if doc['file_path'] else 'N/A'}\")\n        print(f\"    Content: {doc['content'][:150]}...\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"\\nNotice how the ranking changes! Re-ranking often moves more relevant documents to the top.\")\nelse:\n    print(\"No Q&A dataset available for demonstration\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation: With vs Without Re-ranking\n",
    "\n",
    "We'll evaluate the full Q&A dataset with both approaches:\n",
    "\n",
    "1. **Baseline**: Retrieval without re-ranking (top-K by embedding similarity)\n",
    "2. **With Re-ranking**: Retrieve top-20, then re-rank to get final top-K\n",
    "\n",
    "Configuration:\n",
    "- Retrieve: Top-20 candidates\n",
    "- Evaluate: Top-5 after re-ranking\n",
    "- Metrics: All levels including MRR"
   ]
  },
  {
   "cell_type": "code",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "RETRIEVE_K = 20  # Retrieve more candidates for re-ranking\n",
    "EVAL_K = 5       # Evaluate top-5 after re-ranking\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "results_baseline = []\n",
    "results_reranked = []\n",
    "\n",
    "print(f\"Evaluating {len(qa_dataset)} Q&A pairs...\")\n",
    "print(f\"Retrieval: top-{RETRIEVE_K}, Evaluation: top-{EVAL_K}\\n\")\n",
    "\n",
    "for idx, qa_pair in enumerate(tqdm(qa_dataset, desc=\"Evaluating\")):\n",
    "    question = qa_pair[\"question\"]\n",
    "    file_path = qa_pair.get(\"file_path\", \"\")\n",
    "    references_info = qa_pair[\"references\"]\n",
    "    reference_texts = [ref_info[\"text\"] for ref_info in references_info]\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_docs = retrieve_documents(question, k=RETRIEVE_K)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # BASELINE: Use top-K from retrieval directly\n",
    "    # ========================================================================\n",
    "    baseline_docs = retrieved_docs[:EVAL_K]\n",
    "    baseline_texts = [d['content'] for d in baseline_docs]\n",
    "    baseline_file_paths = [d['file_path'] for d in baseline_docs]\n",
    "    \n",
    "    # Compute all metrics\n",
    "    token_metrics = compute_token_metrics(reference_texts, baseline_texts, threshold=THRESHOLD)\n",
    "    passage_metrics = compute_passage_metrics(reference_texts, baseline_texts, threshold=THRESHOLD)\n",
    "    document_metrics = compute_document_metrics(file_path, baseline_file_paths)\n",
    "    mrr_metrics = compute_mrr(reference_texts, baseline_texts, threshold=THRESHOLD)\n",
    "    \n",
    "    results_baseline.append({\n",
    "        \"question\": question,\n",
    "        \"k\": EVAL_K,\n",
    "        # Token-level\n",
    "        \"token_iou\": token_metrics[\"iou\"],\n",
    "        \"token_precision\": token_metrics[\"precision\"],\n",
    "        \"token_recall\": token_metrics[\"recall\"],\n",
    "        \"token_f1\": token_metrics[\"f1\"],\n",
    "        # Passage-level\n",
    "        \"passage_coverage\": passage_metrics[\"coverage\"],\n",
    "        \"passage_accuracy\": passage_metrics[\"accuracy\"],\n",
    "        \"passage_precision\": passage_metrics[\"precision\"],\n",
    "        \"passage_recall\": passage_metrics[\"recall\"],\n",
    "        \"passage_f1\": passage_metrics[\"f1\"],\n",
    "        # Document-level\n",
    "        \"doc_coverage\": document_metrics[\"coverage\"],\n",
    "        \"doc_precision\": document_metrics[\"precision\"],\n",
    "        # Ranking quality (MRR)\n",
    "        \"reciprocal_rank\": mrr_metrics[\"reciprocal_rank\"],\n",
    "        \"first_relevant_rank\": mrr_metrics[\"first_relevant_rank\"]\n",
    "    })\n",
    "    \n",
    "    # ========================================================================\n",
    "    # WITH RE-RANKING: Re-rank all retrieved docs, then take top-K\n",
    "    # ========================================================================\n",
    "    doc_contents = [d['content'] for d in retrieved_docs]\n",
    "    reranked_contents, reranked_scores = reranker.rerank(question, doc_contents)\n",
    "    \n",
    "    # Map back to original documents\n",
    "    content_to_doc = {d['content']: d for d in retrieved_docs}\n",
    "    reranked_docs = [content_to_doc[content] for content in reranked_contents]\n",
    "    \n",
    "    # Take top-K after re-ranking\n",
    "    reranked_docs_topk = reranked_docs[:EVAL_K]\n",
    "    reranked_texts = [d['content'] for d in reranked_docs_topk]\n",
    "    reranked_file_paths = [d['file_path'] for d in reranked_docs_topk]\n",
    "    \n",
    "    # Compute all metrics\n",
    "    token_metrics_rr = compute_token_metrics(reference_texts, reranked_texts, threshold=THRESHOLD)\n",
    "    passage_metrics_rr = compute_passage_metrics(reference_texts, reranked_texts, threshold=THRESHOLD)\n",
    "    document_metrics_rr = compute_document_metrics(file_path, reranked_file_paths)\n",
    "    mrr_metrics_rr = compute_mrr(reference_texts, reranked_texts, threshold=THRESHOLD)\n",
    "    \n",
    "    results_reranked.append({\n",
    "        \"question\": question,\n",
    "        \"k\": EVAL_K,\n",
    "        # Token-level\n",
    "        \"token_iou\": token_metrics_rr[\"iou\"],\n",
    "        \"token_precision\": token_metrics_rr[\"precision\"],\n",
    "        \"token_recall\": token_metrics_rr[\"recall\"],\n",
    "        \"token_f1\": token_metrics_rr[\"f1\"],\n",
    "        # Passage-level\n",
    "        \"passage_coverage\": passage_metrics_rr[\"coverage\"],\n",
    "        \"passage_accuracy\": passage_metrics_rr[\"accuracy\"],\n",
    "        \"passage_precision\": passage_metrics_rr[\"precision\"],\n",
    "        \"passage_recall\": passage_metrics_rr[\"recall\"],\n",
    "        \"passage_f1\": passage_metrics_rr[\"f1\"],\n",
    "        # Document-level\n",
    "        \"doc_coverage\": document_metrics_rr[\"coverage\"],\n",
    "        \"doc_precision\": document_metrics_rr[\"precision\"],\n",
    "        # Ranking quality (MRR)\n",
    "        \"reciprocal_rank\": mrr_metrics_rr[\"reciprocal_rank\"],\n",
    "        \"first_relevant_rank\": mrr_metrics_rr[\"first_relevant_rank\"]\n",
    "    })\n",
    "\n",
    "df_baseline = pd.DataFrame(results_baseline)\n",
    "df_reranked = pd.DataFrame(results_reranked)\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "id": "results",
   "metadata": {},
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EVALUATION RESULTS @ K={EVAL_K}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(\"TOKEN-LEVEL METRICS\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"{'Metric':<20} {'Baseline':>12} {'Re-ranked':>12} {'Improvement':>12}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for metric in ['token_iou', 'token_precision', 'token_recall', 'token_f1']:\n",
    "    baseline_val = df_baseline[metric].mean()\n",
    "    reranked_val = df_reranked[metric].mean()\n",
    "    improvement = reranked_val - baseline_val\n",
    "    print(f\"{metric:<20} {baseline_val:>12.4f} {reranked_val:>12.4f} {improvement:>+12.4f}\")\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(\"PASSAGE-LEVEL METRICS\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"{'Metric':<20} {'Baseline':>12} {'Re-ranked':>12} {'Improvement':>12}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for metric in ['passage_coverage', 'passage_accuracy', 'passage_precision', 'passage_recall', 'passage_f1']:\n",
    "    baseline_val = df_baseline[metric].mean()\n",
    "    reranked_val = df_reranked[metric].mean()\n",
    "    improvement = reranked_val - baseline_val\n",
    "    print(f\"{metric:<20} {baseline_val:>12.4f} {reranked_val:>12.4f} {improvement:>+12.4f}\")\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(\"DOCUMENT-LEVEL METRICS\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"{'Metric':<20} {'Baseline':>12} {'Re-ranked':>12} {'Improvement':>12}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for metric in ['doc_coverage', 'doc_precision']:\n",
    "    baseline_val = df_baseline[metric].mean()\n",
    "    reranked_val = df_reranked[metric].mean()\n",
    "    improvement = reranked_val - baseline_val\n",
    "    print(f\"{metric:<20} {baseline_val:>12.4f} {reranked_val:>12.4f} {improvement:>+12.4f}\")\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(\"RANKING QUALITY METRICS\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"{'Metric':<20} {'Baseline':>12} {'Re-ranked':>12} {'Improvement':>12}\")\n",
    "print(f\"{'-'*60}\")\n",
    "baseline_mrr = df_baseline['reciprocal_rank'].mean()\n",
    "reranked_mrr = df_reranked['reciprocal_rank'].mean()\n",
    "mrr_improvement = reranked_mrr - baseline_mrr\n",
    "print(f\"{'MRR':<20} {baseline_mrr:>12.4f} {reranked_mrr:>12.4f} {mrr_improvement:>+12.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Overall summary\n",
    "print(f\"\\nOVERALL SUMMARY:\")\n",
    "print(f\"  Re-ranking improves:\")\n",
    "print(f\"    - Passage Coverage: {(df_reranked['passage_coverage'].mean() - df_baseline['passage_coverage'].mean())*100:+.1f}%\")\n",
    "print(f\"    - MRR: {mrr_improvement:+.4f} (higher is better)\")\n",
    "print(f\"    - Document Precision: {(df_reranked['doc_precision'].mean() - df_baseline['doc_precision'].mean())*100:+.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "rank-distribution-header",
   "metadata": {},
   "source": [
    "## Rank Distribution Analysis\n",
    "\n",
    "Where do relevant documents appear in the ranking?"
   ]
  },
  {
   "cell_type": "code",
   "id": "rank-distribution",
   "metadata": {},
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RANK DISTRIBUTION: Where do relevant documents appear?\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Baseline rank distribution\n",
    "baseline_ranks = df_baseline['first_relevant_rank'].dropna()\n",
    "reranked_ranks = df_reranked['first_relevant_rank'].dropna()\n",
    "\n",
    "print(f\"\\nBASELINE (No re-ranking):\")\n",
    "if len(baseline_ranks) > 0:\n",
    "    rank_counts_baseline = baseline_ranks.value_counts().sort_index()\n",
    "    for rank, count in rank_counts_baseline.items():\n",
    "        pct = (count / len(df_baseline)) * 100\n",
    "        print(f\"  Rank {int(rank)}: {count} queries ({pct:.1f}%)\")\n",
    "    print(f\"  No relevant doc found: {len(df_baseline) - len(baseline_ranks)} queries ({(len(df_baseline) - len(baseline_ranks))/len(df_baseline)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No relevant documents found\")\n",
    "\n",
    "print(f\"\\nWITH RE-RANKING:\")\n",
    "if len(reranked_ranks) > 0:\n",
    "    rank_counts_reranked = reranked_ranks.value_counts().sort_index()\n",
    "    for rank, count in rank_counts_reranked.items():\n",
    "        pct = (count / len(df_reranked)) * 100\n",
    "        print(f\"  Rank {int(rank)}: {count} queries ({pct:.1f}%)\")\n",
    "    print(f\"  No relevant doc found: {len(df_reranked) - len(reranked_ranks)} queries ({(len(df_reranked) - len(reranked_ranks))/len(df_reranked)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No relevant documents found\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\\nKey Insight: Re-ranking should move more relevant documents to Rank 1!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Re-ranking provides significant improvements to RAG retrieval quality:\n",
    "\n",
    "1. **Better Ranking Quality (MRR)**\n",
    "   - Moves relevant documents closer to the top\n",
    "   - Higher MRR = relevant docs appear earlier in results\n",
    "   - Critical for user experience and LLM context\n",
    "\n",
    "2. **Improved Passage Coverage**\n",
    "   - More reference passages found in top-K results\n",
    "   - Better recall of relevant information\n",
    "\n",
    "3. **Higher Document Precision**\n",
    "   - More focused results from source documents\n",
    "   - Less noise from irrelevant documents\n",
    "\n",
    "### When to Use Re-ranking\n",
    "\n",
    "**Use re-ranking when:**\n",
    "- Precision is critical (e.g., question answering)\n",
    "- You need the best documents in top positions\n",
    "- You have computational resources for the second stage\n",
    "- Quality is more important than speed\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
